{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extract Bag of Words (BoW) Features from Course Textual Content**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of recommender systems is to help users find items they potentially interested in. Depending on the recommendation tasks, an item can be a movie, a restaurant, or, in our case, an online course. \n",
    "\n",
    "Machine learning algorithms cannot work on an item directly so we first need to extract features and represent the items mathematically, i.e., with a feature vector.\n",
    "\n",
    "Many items are often described by text so they are associated with textual data, such as the titles and descriptions of a movie or course. Since machine learning algorithms can not process textual data directly, we need to transform the raw text into numeric feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/extract_textual_features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be learning to extract the bag of words (BoW) features from course titles and descriptions. The BoW feature is a simple but effective feature characterizing textual data and is widely used in many textual machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract Bag of Words (BoW) features from course titles and descriptions\n",
    "* Build a course BoW dataset to be used for building a content-based recommender system later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and setup the lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install and import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.6.7 in /opt/conda/lib/python3.11/site-packages (3.6.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk==3.6.7) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk==3.6.7) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk==3.6.7) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk==3.6.7) (4.66.4)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: scipy==1.10 in /opt/conda/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scipy==1.10) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.6.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.6.7\n",
    "!pip install gensim\n",
    "!pip install scipy==1.10\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW features are essentially the counts or frequencies of each word that appears in a text (string). Let's illustrate it with some simple examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two course descriptions as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "course1 = \"this is an introduction data science course which introduces data science to beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "course2 = \"machine learning for beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an introduction data science course which introduces data science to beginners',\n",
       " 'machine learning for beginners']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = [course1, course2]\n",
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to split the two strings into words (tokens). A token in the text processing context means the smallest unit of text such as a word, a symbol/punctuation, or a phrase, etc. The process to transform a string into a collection of tokens is called `tokenization`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common way to do ```tokenization``` is to use the Python built-in `split()` method of the `str` class.  However, in this lab, we want to leverage the `nltk` (Natural Language Toolkit) package, which is probably the most commonly used package to process text or natural language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " More specifically, we will use the ```word_tokenize()``` method on the content of course (string):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the two courses\n",
    "tokenized_courses = [word_tokenize(course) for course in courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this',\n",
       "  'is',\n",
       "  'an',\n",
       "  'introduction',\n",
       "  'data',\n",
       "  'science',\n",
       "  'course',\n",
       "  'which',\n",
       "  'introduces',\n",
       "  'data',\n",
       "  'science',\n",
       "  'to',\n",
       "  'beginners'],\n",
       " ['machine', 'learning', 'for', 'beginners']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the cell output, two courses have been tokenized and turned into two token arrays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create a token dictionary to index all tokens. Basically, we want to assign a key/index for each token. One way to index tokens is to use the `gensim` package which is another popular package for processing textual data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a token dictionary for the two courses\n",
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 0, 'beginners': 1, 'course': 2, 'data': 3, 'introduces': 4, 'introduction': 5, 'is': 6, 'science': 7, 'this': 8, 'to': 9, 'which': 10, 'for': 11, 'learning': 12, 'machine': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokens_dict.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the token dictionary, we can easily count each token in the two example courses and output two BoW feature vectors. However, more conveniently, the `gensim` package provides us a `doc2bow` method to generate BoW features out-of-box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BoW features for each course\n",
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 2),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1)],\n",
       " [(1, 1), (11, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs two BoW arrays where each element is a tuple, e.g., (0, 1) and (7, 2). The first element of the tuple is the token ID and the second element is its count. So `(0, 1)` means `(``an``, 1)` and `(7, 2)` means `(``science``, 2)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following code snippet to print each token and its count:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words for course 0:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 1:\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n"
     ]
    }
   ],
   "source": [
    "# Enumerate through each course and its bag-of-words representation\n",
    "for course_idx, course_bow in enumerate(courses_bow):\n",
    "    # Print the index of the current course and a label\n",
    "    print(f\"Bag of words for course {course_idx}:\")\n",
    "    # For each token index, print its bow value (word count)\n",
    "    for token_index, token_bow in course_bow:\n",
    "        # Retrieve the token from the tokens dictionary based on its index\n",
    "        token = tokens_dict.get(token_index)\n",
    "        # Print the token and its bag-of-words value\n",
    "        print(f\"--Token: '{token}', Count:{token_bow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we turn to the long list into a horizontal feature vectors, we can see the two courses become two numerical feature vectors:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/bow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document may contain tens of thousands of words which makes the dimension of the BoW feature vector huge. To reduce the dimensionality, one common way is to filter the relatively meaningless tokens such as stop words or sometimes add position and adjective words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the english stop words provided in `nltk`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can filter those English stop words from the tokens in course1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'which',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'to',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens in course 1\n",
    "tokenized_courses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tokens = [w for w in tokenized_courses[0] if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the number of tokens for ```course1``` has been reduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way is to only keep nouns in the text. We can use the `nltk.pos_tag()` method to analyze the part of speech (POS) and annotate each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('introduction', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('course', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('introduces', 'VBZ'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('beginners', 'NNS')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokenized_courses[0])\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see [`introduction`, `data`, `science`, `course`, `beginners`] are all of the nouns and we may keep them in the BoW feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Extract BoW features for course textual content and build a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you have learned what a BoW feature is, so let's start extracting BoW features from some real course textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv\"\n",
    "course_content_df = pd.read_csv(course_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COURSE_ID                                               ML0201EN\n",
       "TITLE          robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION    have fun with iot and learn along the way  if ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course content dataset has three columns `COURSE_ID`, `TITLE`, and `DESCRIPTION`. `TITLE` and `DESCRIPTION` are all text upon which we want to extract BoW features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join those two text columns together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge TITLE and DESCRIPTION title\n",
    "course_content_df['course_texts'] = course_content_df[['TITLE', 'DESCRIPTION']].agg(' '.join, axis=1)\n",
    "course_content_df = course_content_df.reset_index()\n",
    "course_content_df['index'] = course_content_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                           0\n",
       "COURSE_ID                                                ML0201EN\n",
       "TITLE           robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION     have fun with iot and learn along the way  if ...\n",
       "course_texts    robots are coming  build iot apps with watson ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `tokenize_course()` method  to tokenize the course content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_course(course, keep_only_nouns=True):\n",
    "    # Get English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Tokenize the course text\n",
    "    word_tokens = word_tokenize(course)\n",
    "    # Remove English stop words and numbers\n",
    "    word_tokens = [w for w in word_tokens if (not w.lower() in stop_words) and (not w.isnumeric())]\n",
    "    # Only keep nouns \n",
    "    if keep_only_nouns:\n",
    "        # Define a filter list of non-noun POS tags\n",
    "        filter_list = ['WDT', 'WP', 'WRB', 'FW', 'IN', 'JJR', 'JJS', 'MD', 'PDT', 'POS', 'PRP', 'RB', 'RBR', 'RBS',\n",
    "                       'RP']\n",
    "        # Tag the word tokens with POS tags\n",
    "        tags = nltk.pos_tag(word_tokens)\n",
    "        # Filter out non-nouns based on POS tags\n",
    "        word_tokens = [word for word, pos in tags if pos not in filter_list]\n",
    "\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the first course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robots are coming  build iot apps with watson  swift  and node red have fun with iot and learn along the way  if you re a swift developer and want to learn more about iot and watson ai services in the cloud  raspberry pi   and node red  you ve found the right place  you ll build iot apps to read temperature data  take pictures with a raspcam  use ai to recognize the objects in those pictures  and program an irobot create 2 robot  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_course = course_content_df.iloc[0, :]['course_texts']\n",
    "a_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robots',\n",
       " 'coming',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'watson',\n",
       " 'swift',\n",
       " 'red',\n",
       " 'fun',\n",
       " 'iot',\n",
       " 'learn',\n",
       " 'way',\n",
       " 'swift',\n",
       " 'developer',\n",
       " 'want',\n",
       " 'learn',\n",
       " 'iot',\n",
       " 'watson',\n",
       " 'ai',\n",
       " 'services',\n",
       " 'cloud',\n",
       " 'raspberry',\n",
       " 'pi',\n",
       " 'node',\n",
       " 'red',\n",
       " 'found',\n",
       " 'place',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'read',\n",
       " 'temperature',\n",
       " 'data',\n",
       " 'take',\n",
       " 'pictures',\n",
       " 'raspcam',\n",
       " 'use',\n",
       " 'ai',\n",
       " 'recognize',\n",
       " 'objects',\n",
       " 'pictures',\n",
       " 'program',\n",
       " 'irobot',\n",
       " 'create',\n",
       " 'robot']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_course(a_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will need to write some code snippets to generate the BoW features for each course. Let's start by tokenzing all courses in the `courses_df`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Use provided tokenize_course() method to tokenize all courses in courses_df['course_texts']._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses with Tokenized Texts:\n",
      "       COURSE_ID                                              TITLE  \\\n",
      "0      ML0201EN  robots are coming  build iot apps with watson ...   \n",
      "1      ML0122EN                accelerating deep learning with gpu   \n",
      "2    GPXX0ZG0EN  consuming restful services using the reactive ...   \n",
      "3      RP0105EN         analyzing big data in r using apache spark   \n",
      "4    GPXX0Z2PEN  containerizing  packaging  and running a sprin...   \n",
      "..          ...                                                ...   \n",
      "302  excourse89                       javascript  jquery  and json   \n",
      "303  excourse90  programming foundations with javascript  html ...   \n",
      "304  excourse91               front end web development with react   \n",
      "305  excourse92                    introduction to web development   \n",
      "306  excourse93           interactivity with javascript and jquery   \n",
      "\n",
      "                                                tokens  \n",
      "0    [robots, are, coming, build, iot, apps, with, ...  \n",
      "1    [accelerating, deep, learning, with, gpu, trai...  \n",
      "2    [consuming, restful, services, using, the, rea...  \n",
      "3    [analyzing, big, data, in, r, using, apache, s...  \n",
      "4    [containerizing, packaging, and, running, a, s...  \n",
      "..                                                 ...  \n",
      "302  [javascript, jquery, and, json, in, this, cour...  \n",
      "303  [programming, foundations, with, javascript, h...  \n",
      "304  [front, end, web, development, with, react, th...  \n",
      "305  [introduction, to, web, development, this, cou...  \n",
      "306  [interactivity, with, javascript, and, jquery,...  \n",
      "\n",
      "[307 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "# Assuming the tokenize_course function is defined elsewhere\n",
    "def tokenize_course(text, lowercase):\n",
    "    # Sample implementation for demonstration purposes\n",
    "    import re\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # Simple tokenization\n",
    "    return tokens\n",
    "\n",
    "# Tokenize each course text\n",
    "course_content_df['tokens'] = course_content_df['course_texts'].apply(lambda text: tokenize_course(text, True))\n",
    "\n",
    "# Display the DataFrame with tokens\n",
    "print(\"Courses with Tokenized Texts:\\n\", course_content_df[['COURSE_ID', 'TITLE', 'tokens']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "\n",
    "Use `tokenize_course(text, True)` command to tokenize each text in `courses_df['course_texts']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create a token dictionary `tokens_dict`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Use gensim.corpora.Dictionary(tokenized_courses) to create a token dictionary._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses with Tokenized Texts:\n",
      "       COURSE_ID                                              TITLE  \\\n",
      "0      ML0201EN  robots are coming  build iot apps with watson ...   \n",
      "1      ML0122EN                accelerating deep learning with gpu   \n",
      "2    GPXX0ZG0EN  consuming restful services using the reactive ...   \n",
      "3      RP0105EN         analyzing big data in r using apache spark   \n",
      "4    GPXX0Z2PEN  containerizing  packaging  and running a sprin...   \n",
      "..          ...                                                ...   \n",
      "302  excourse89                       javascript  jquery  and json   \n",
      "303  excourse90  programming foundations with javascript  html ...   \n",
      "304  excourse91               front end web development with react   \n",
      "305  excourse92                    introduction to web development   \n",
      "306  excourse93           interactivity with javascript and jquery   \n",
      "\n",
      "                                                tokens  \n",
      "0    [robots, are, coming, build, iot, apps, with, ...  \n",
      "1    [accelerating, deep, learning, with, gpu, trai...  \n",
      "2    [consuming, restful, services, using, the, rea...  \n",
      "3    [analyzing, big, data, in, r, using, apache, s...  \n",
      "4    [containerizing, packaging, and, running, a, s...  \n",
      "..                                                 ...  \n",
      "302  [javascript, jquery, and, json, in, this, cour...  \n",
      "303  [programming, foundations, with, javascript, h...  \n",
      "304  [front, end, web, development, with, react, th...  \n",
      "305  [introduction, to, web, development, this, cou...  \n",
      "306  [interactivity, with, javascript, and, jquery,...  \n",
      "\n",
      "[307 rows x 3 columns]\n",
      "Token Dictionary:\n",
      "ID: 0, Token: 2\n",
      "ID: 1, Token: a\n",
      "ID: 2, Token: about\n",
      "ID: 3, Token: ai\n",
      "ID: 4, Token: along\n",
      "ID: 5, Token: an\n",
      "ID: 6, Token: and\n",
      "ID: 7, Token: apps\n",
      "ID: 8, Token: are\n",
      "ID: 9, Token: build\n",
      "ID: 10, Token: cloud\n",
      "ID: 11, Token: coming\n",
      "ID: 12, Token: create\n",
      "ID: 13, Token: data\n",
      "ID: 14, Token: developer\n",
      "ID: 15, Token: found\n",
      "ID: 16, Token: fun\n",
      "ID: 17, Token: have\n",
      "ID: 18, Token: if\n",
      "ID: 19, Token: in\n",
      "ID: 20, Token: iot\n",
      "ID: 21, Token: irobot\n",
      "ID: 22, Token: learn\n",
      "ID: 23, Token: ll\n",
      "ID: 24, Token: more\n",
      "ID: 25, Token: node\n",
      "ID: 26, Token: objects\n",
      "ID: 27, Token: pi\n",
      "ID: 28, Token: pictures\n",
      "ID: 29, Token: place\n",
      "ID: 30, Token: program\n",
      "ID: 31, Token: raspberry\n",
      "ID: 32, Token: raspcam\n",
      "ID: 33, Token: re\n",
      "ID: 34, Token: read\n",
      "ID: 35, Token: recognize\n",
      "ID: 36, Token: red\n",
      "ID: 37, Token: right\n",
      "ID: 38, Token: robot\n",
      "ID: 39, Token: robots\n",
      "ID: 40, Token: services\n",
      "ID: 41, Token: swift\n",
      "ID: 42, Token: take\n",
      "ID: 43, Token: temperature\n",
      "ID: 44, Token: the\n",
      "ID: 45, Token: those\n",
      "ID: 46, Token: to\n",
      "ID: 47, Token: use\n",
      "ID: 48, Token: ve\n",
      "ID: 49, Token: want\n",
      "ID: 50, Token: watson\n",
      "ID: 51, Token: way\n",
      "ID: 52, Token: with\n",
      "ID: 53, Token: you\n",
      "ID: 54, Token: accelerate\n",
      "ID: 55, Token: accelerated\n",
      "ID: 56, Token: accelerating\n",
      "ID: 57, Token: also\n",
      "ID: 58, Token: analyze\n",
      "ID: 59, Token: as\n",
      "ID: 60, Token: based\n",
      "ID: 61, Token: be\n",
      "ID: 62, Token: been\n",
      "ID: 63, Token: benefit\n",
      "ID: 64, Token: but\n",
      "ID: 65, Token: caffe\n",
      "ID: 66, Token: can\n",
      "ID: 67, Token: case\n",
      "ID: 68, Token: chips\n",
      "ID: 69, Token: classification\n",
      "ID: 70, Token: comfortable\n",
      "ID: 71, Token: complex\n",
      "ID: 72, Token: computations\n",
      "ID: 73, Token: convolutional\n",
      "ID: 74, Token: course\n",
      "ID: 75, Token: datasets\n",
      "ID: 76, Token: deep\n",
      "ID: 77, Token: dependencies\n",
      "ID: 78, Token: deploy\n",
      "ID: 79, Token: designed\n",
      "ID: 80, Token: feel\n",
      "ID: 81, Token: for\n",
      "ID: 82, Token: google\n",
      "ID: 83, Token: gpu\n",
      "ID: 84, Token: hardware\n",
      "ID: 85, Token: has\n",
      "ID: 86, Token: house\n",
      "ID: 87, Token: how\n",
      "ID: 88, Token: ibm\n",
      "ID: 89, Token: images\n",
      "ID: 90, Token: including\n",
      "ID: 91, Token: inference\n",
      "ID: 92, Token: is\n",
      "ID: 93, Token: it\n",
      "ID: 94, Token: large\n",
      "ID: 95, Token: learning\n",
      "ID: 96, Token: libraries\n",
      "ID: 97, Token: machine\n",
      "ID: 98, Token: may\n",
      "ID: 99, Token: might\n",
      "ID: 100, Token: models\n",
      "ID: 101, Token: need\n",
      "ID: 102, Token: needs\n",
      "ID: 103, Token: network\n",
      "ID: 104, Token: networks\n",
      "ID: 105, Token: neural\n",
      "ID: 106, Token: not\n",
      "ID: 107, Token: nvidia\n",
      "ID: 108, Token: of\n",
      "ID: 109, Token: on\n",
      "ID: 110, Token: one\n",
      "ID: 111, Token: or\n",
      "ID: 112, Token: overcome\n",
      "ID: 113, Token: platform\n",
      "ID: 114, Token: popular\n",
      "ID: 115, Token: power\n",
      "ID: 116, Token: preferring\n",
      "ID: 117, Token: premise\n",
      "ID: 118, Token: problem\n",
      "ID: 119, Token: problems\n",
      "ID: 120, Token: processing\n",
      "ID: 121, Token: proven\n",
      "ID: 122, Token: public\n",
      "ID: 123, Token: recently\n",
      "ID: 124, Token: reduce\n",
      "ID: 125, Token: s\n",
      "ID: 126, Token: scalability\n",
      "ID: 127, Token: scaling\n",
      "ID: 128, Token: sensitiveand\n",
      "ID: 129, Token: several\n",
      "ID: 130, Token: significantly\n",
      "ID: 131, Token: solution\n",
      "ID: 132, Token: specifically\n",
      "ID: 133, Token: such\n",
      "ID: 134, Token: support\n",
      "ID: 135, Token: supports\n",
      "ID: 136, Token: system\n",
      "ID: 137, Token: systems\n",
      "ID: 138, Token: takes\n",
      "ID: 139, Token: tensor\n",
      "ID: 140, Token: tensorflow\n",
      "ID: 141, Token: that\n",
      "ID: 142, Token: theano\n",
      "ID: 143, Token: these\n",
      "ID: 144, Token: this\n",
      "ID: 145, Token: time\n",
      "ID: 146, Token: torch\n",
      "ID: 147, Token: tpu\n",
      "ID: 148, Token: trained\n",
      "ID: 149, Token: training\n",
      "ID: 150, Token: understand\n",
      "ID: 151, Token: unit\n",
      "ID: 152, Token: uploading\n",
      "ID: 153, Token: videos\n",
      "ID: 154, Token: well\n",
      "ID: 155, Token: what\n",
      "ID: 156, Token: will\n",
      "ID: 157, Token: your\n",
      "ID: 158, Token: asynchronously\n",
      "ID: 159, Token: client\n",
      "ID: 160, Token: consuming\n",
      "ID: 161, Token: http\n",
      "ID: 162, Token: invoke\n",
      "ID: 163, Token: jax\n",
      "ID: 164, Token: microservices\n",
      "ID: 165, Token: over\n",
      "ID: 166, Token: reactive\n",
      "ID: 167, Token: restful\n",
      "ID: 168, Token: rs\n",
      "ID: 169, Token: using\n",
      "ID: 170, Token: analysis\n",
      "ID: 171, Token: analyzing\n",
      "ID: 172, Token: apache\n",
      "ID: 173, Token: api\n",
      "ID: 174, Token: big\n",
      "ID: 175, Token: cluster\n",
      "ID: 176, Token: computing\n",
      "ID: 177, Token: distributed\n",
      "ID: 178, Token: enables\n",
      "ID: 179, Token: familiar\n",
      "ID: 180, Token: frame\n",
      "ID: 181, Token: framework\n",
      "ID: 182, Token: performing\n",
      "ID: 183, Token: provides\n",
      "ID: 184, Token: r\n",
      "ID: 185, Token: scale\n",
      "ID: 186, Token: spark\n",
      "ID: 187, Token: sparkr\n",
      "ID: 188, Token: structured\n",
      "ID: 189, Token: syntax\n",
      "ID: 190, Token: used\n",
      "ID: 191, Token: users\n",
      "ID: 192, Token: application\n",
      "ID: 193, Token: boot\n",
      "ID: 194, Token: containerize\n",
      "ID: 195, Token: containerizing\n",
      "ID: 196, Token: liberty\n",
      "ID: 197, Token: modification\n",
      "ID: 198, Token: open\n",
      "ID: 199, Token: package\n",
      "ID: 200, Token: packaging\n",
      "ID: 201, Token: run\n",
      "ID: 202, Token: running\n",
      "ID: 203, Token: server\n",
      "ID: 204, Token: spring\n",
      "ID: 205, Token: without\n",
      "ID: 206, Token: conference\n",
      "ID: 207, Token: introduction\n",
      "ID: 208, Token: native\n",
      "ID: 209, Token: security\n",
      "ID: 210, Token: bootcamp\n",
      "ID: 211, Token: by\n",
      "ID: 212, Token: day\n",
      "ID: 213, Token: intensive\n",
      "ID: 214, Token: multi\n",
      "ID: 215, Token: offered\n",
      "ID: 216, Token: person\n",
      "ID: 217, Token: proffesors\n",
      "ID: 218, Token: science\n",
      "ID: 219, Token: university\n",
      "ID: 220, Token: containers\n",
      "ID: 221, Token: development\n",
      "ID: 222, Token: docker\n",
      "ID: 223, Token: iterative\n",
      "ID: 224, Token: 1\n",
      "ID: 225, Token: scorm\n",
      "ID: 226, Token: scron\n",
      "ID: 227, Token: test\n",
      "ID: 228, Token: basic\n",
      "ID: 229, Token: collections\n",
      "ID: 230, Token: creating\n",
      "ID: 231, Token: database\n",
      "ID: 232, Token: document\n",
      "ID: 233, Token: doing\n",
      "ID: 234, Token: first\n",
      "ID: 235, Token: get\n",
      "ID: 236, Token: guided\n",
      "ID: 237, Token: management\n",
      "ID: 238, Token: mongodb\n",
      "ID: 239, Token: project\n",
      "ID: 240, Token: started\n",
      "ID: 241, Token: working\n",
      "ID: 242, Token: arquillian\n",
      "ID: 243, Token: container\n",
      "ID: 244, Token: develop\n",
      "ID: 245, Token: managed\n",
      "ID: 246, Token: testing\n",
      "ID: 247, Token: tests\n",
      "ID: 248, Token: aiops\n",
      "ID: 249, Token: attending\n",
      "ID: 250, Token: comprehensive\n",
      "ID: 251, Token: demonstrate\n",
      "ID: 252, Token: digital\n",
      "ID: 253, Token: essentials\n",
      "ID: 254, Token: experience\n",
      "ID: 255, Token: from\n",
      "ID: 256, Token: hands\n",
      "ID: 257, Token: integration\n",
      "ID: 258, Token: pak\n",
      "ID: 259, Token: received\n",
      "ID: 260, Token: short\n",
      "ID: 261, Token: analytics\n",
      "ID: 262, Token: assemble\n",
      "ID: 263, Token: base\n",
      "ID: 264, Token: basics\n",
      "ID: 265, Token: dataset\n",
      "ID: 266, Token: explore\n",
      "ID: 267, Token: fundamentals\n",
      "ID: 268, Token: further\n",
      "ID: 269, Token: improve\n",
      "ID: 270, Token: line\n",
      "ID: 271, Token: media\n",
      "ID: 272, Token: refine\n",
      "ID: 273, Token: reports\n",
      "ID: 274, Token: service\n",
      "ID: 275, Token: setup\n",
      "ID: 276, Token: smart\n",
      "ID: 277, Token: social\n",
      "ID: 278, Token: suggestions\n",
      "ID: 279, Token: teaches\n",
      "ID: 280, Token: topic\n",
      "ID: 281, Token: within\n",
      "ID: 282, Token: advance\n",
      "ID: 283, Token: professors\n",
      "ID: 284, Token: python\n",
      "ID: 285, Token: algorithm\n",
      "ID: 286, Token: algorithmic\n",
      "ID: 287, Token: cryptocurrency\n",
      "ID: 288, Token: dive\n",
      "ID: 289, Token: do\n",
      "ID: 290, Token: earning\n",
      "ID: 291, Token: good\n",
      "ID: 292, Token: into\n",
      "ID: 293, Token: just\n",
      "ID: 294, Token: money\n",
      "ID: 295, Token: sleep\n",
      "ID: 296, Token: sound\n",
      "ID: 297, Token: too\n",
      "ID: 298, Token: trading\n",
      "ID: 299, Token: true\n",
      "ID: 300, Token: while\n",
      "ID: 301, Token: world\n",
      "ID: 302, Token: access\n",
      "ID: 303, Token: control\n",
      "ID: 304, Token: eclipse\n",
      "ID: 305, Token: java\n",
      "ID: 306, Token: json\n",
      "ID: 307, Token: jwt\n",
      "ID: 308, Token: microprofile\n",
      "ID: 309, Token: role\n",
      "ID: 310, Token: securing\n",
      "ID: 311, Token: token\n",
      "ID: 312, Token: user\n",
      "ID: 313, Token: web\n",
      "ID: 314, Token: customize\n",
      "ID: 315, Token: enable\n",
      "ID: 316, Token: enabling\n",
      "ID: 317, Token: methods\n",
      "ID: 318, Token: non\n",
      "ID: 319, Token: opentracing\n",
      "ID: 320, Token: tracing\n",
      "ID: 321, Token: zipkin\n",
      "ID: 322, Token: already\n",
      "ID: 323, Token: another\n",
      "ID: 324, Token: applying\n",
      "ID: 325, Token: architecture\n",
      "ID: 326, Token: chance\n",
      "ID: 327, Token: common\n",
      "ID: 328, Token: connect\n",
      "ID: 329, Token: cover\n",
      "ID: 330, Token: curve\n",
      "ID: 331, Token: different\n",
      "ID: 332, Token: easy\n",
      "ID: 333, Token: exercises\n",
      "ID: 334, Token: existing\n",
      "ID: 335, Token: explain\n",
      "ID: 336, Token: file\n",
      "ID: 337, Token: formats\n",
      "ID: 338, Token: hadoop\n",
      "ID: 339, Token: help\n",
      "ID: 340, Token: here\n",
      "ID: 341, Token: lab\n",
      "ID: 342, Token: list\n",
      "ID: 343, Token: load\n",
      "ID: 344, Token: no\n",
      "ID: 345, Token: orc\n",
      "ID: 346, Token: parquet\n",
      "ID: 347, Token: proprietary\n",
      "ID: 348, Token: relational\n",
      "ID: 349, Token: sample\n",
      "ID: 350, Token: schemas\n",
      "ID: 351, Token: see\n",
      "ID: 352, Token: show\n",
      "ID: 353, Token: some\n",
      "ID: 354, Token: sql\n",
      "ID: 355, Token: storage\n",
      "ID: 356, Token: supported\n",
      "ID: 357, Token: table\n",
      "ID: 358, Token: their\n",
      "ID: 359, Token: then\n",
      "ID: 360, Token: there\n",
      "ID: 361, Token: tool\n",
      "ID: 362, Token: types\n",
      "ID: 363, Token: work\n",
      "ID: 364, Token: äì\n",
      "ID: 365, Token: hybrid\n",
      "ID: 366, Token: pipelines\n",
      "ID: 367, Token: dataops\n",
      "ID: 368, Token: methodology\n",
      "ID: 369, Token: ops\n",
      "ID: 370, Token: introduce\n",
      "ID: 371, Token: journey\n",
      "ID: 372, Token: jumpstart\n",
      "ID: 373, Token: any\n",
      "ID: 374, Token: concepts\n",
      "ID: 375, Token: contribute\n",
      "ID: 376, Token: introduces\n",
      "ID: 377, Token: key\n",
      "ID: 378, Token: processes\n",
      "ID: 379, Token: software\n",
      "ID: 380, Token: source\n",
      "ID: 381, Token: tools\n",
      "ID: 382, Token: cloudpak\n",
      "ID: 383, Token: end\n",
      "ID: 384, Token: applications\n",
      "ID: 385, Token: artificial\n",
      "ID: 386, Token: everyone\n",
      "ID: 387, Token: intelligence\n",
      "ID: 388, Token: its\n",
      "ID: 389, Token: master\n",
      "ID: 390, Token: understanding\n",
      "ID: 391, Token: composed\n",
      "ID: 392, Token: coupled\n",
      "ID: 393, Token: functions\n",
      "ID: 394, Token: i\n",
      "ID: 395, Token: include\n",
      "ID: 396, Token: like\n",
      "ID: 397, Token: loosely\n",
      "ID: 398, Token: microservice\n",
      "ID: 399, Token: serverless\n",
      "ID: 400, Token: teach\n",
      "ID: 401, Token: which\n",
      "ID: 402, Token: customer\n",
      "ID: 403, Token: moddels\n",
      "ID: 404, Token: predict\n",
      "ID: 405, Token: predicting\n",
      "ID: 406, Token: satisfaction\n",
      "ID: 407, Token: satisfactions\n",
      "ID: 408, Token: bean\n",
      "ID: 409, Token: constraints\n",
      "ID: 410, Token: input\n",
      "ID: 411, Token: javabeans\n",
      "ID: 412, Token: validate\n",
      "ID: 413, Token: validating\n",
      "ID: 414, Token: validation\n",
      "ID: 415, Token: 100\n",
      "ID: 416, Token: algorithms\n",
      "ID: 417, Token: alternative\n",
      "ID: 418, Token: amounts\n",
      "ID: 419, Token: apis\n",
      "ID: 420, Token: around\n",
      "ID: 421, Token: at\n",
      "ID: 422, Token: built\n",
      "ID: 423, Token: cannot\n",
      "ID: 424, Token: cassandra\n",
      "ID: 425, Token: combines\n",
      "ID: 426, Token: diverse\n",
      "ID: 427, Token: ease\n",
      "ID: 428, Token: engine\n",
      "ID: 429, Token: fast\n",
      "ID: 430, Token: faster\n",
      "ID: 431, Token: getting\n",
      "ID: 432, Token: handle\n",
      "ID: 433, Token: hbase\n",
      "ID: 434, Token: hdfs\n",
      "ID: 435, Token: interactive\n",
      "ID: 436, Token: latency\n",
      "ID: 437, Token: lightning\n",
      "ID: 438, Token: low\n",
      "ID: 439, Token: makers\n",
      "ID: 440, Token: map\n",
      "ID: 441, Token: memory\n",
      "ID: 442, Token: mesos\n",
      "ID: 443, Token: mining\n",
      "ID: 444, Token: performs\n",
      "ID: 445, Token: provide\n",
      "ID: 446, Token: range\n",
      "ID: 447, Token: requires\n",
      "ID: 448, Token: runs\n",
      "ID: 449, Token: s3\n",
      "ID: 450, Token: same\n",
      "ID: 451, Token: scala\n",
      "ID: 452, Token: scenarios\n",
      "ID: 453, Token: seamlessly\n",
      "ID: 454, Token: sources\n",
      "ID: 455, Token: speed\n",
      "ID: 456, Token: speeds\n",
      "ID: 457, Token: standalone\n",
      "ID: 458, Token: streaming\n",
      "ID: 459, Token: than\n",
      "ID: 460, Token: times\n",
      "ID: 461, Token: together\n",
      "ID: 462, Token: top\n",
      "ID: 463, Token: typical\n",
      "ID: 464, Token: up\n",
      "ID: 465, Token: wide\n",
      "ID: 466, Token: company\n",
      "ID: 467, Token: financial\n",
      "ID: 468, Token: performance\n",
      "ID: 469, Token: db2\n",
      "ID: 470, Token: clustering\n",
      "ID: 471, Token: investment\n",
      "ID: 472, Token: portfolio\n",
      "ID: 473, Token: creation\n",
      "ID: 474, Token: lamp\n",
      "ID: 475, Token: linux\n",
      "ID: 476, Token: mysql\n",
      "ID: 477, Token: php\n",
      "ID: 478, Token: scripting\n",
      "ID: 479, Token: stack\n",
      "ID: 480, Token: through\n",
      "ID: 481, Token: tutorial\n",
      "ID: 482, Token: ubuntu\n",
      "ID: 483, Token: virtual\n",
      "ID: 484, Token: walks\n",
      "ID: 485, Token: game\n",
      "ID: 486, Token: javascript\n",
      "ID: 487, Token: language\n",
      "ID: 488, Token: paper\n",
      "ID: 489, Token: programming\n",
      "ID: 490, Token: recreating\n",
      "ID: 491, Token: rock\n",
      "ID: 492, Token: scissors\n",
      "ID: 493, Token: versatile\n",
      "ID: 494, Token: databases\n",
      "ID: 495, Token: intent\n",
      "ID: 496, Token: magic\n",
      "ID: 497, Token: modify\n",
      "ID: 498, Token: perform\n",
      "ID: 499, Token: query\n",
      "ID: 500, Token: simple\n",
      "ID: 501, Token: unlock\n",
      "ID: 502, Token: update\n",
      "ID: 503, Token: visualizations\n",
      "ID: 504, Token: acceleration\n",
      "ID: 505, Token: blu\n",
      "ID: 506, Token: c\n",
      "ID: 507, Token: environment\n",
      "ID: 508, Token: express\n",
      "ID: 509, Token: focus\n",
      "ID: 510, Token: interface\n",
      "ID: 511, Token: mangement\n",
      "ID: 512, Token: our\n",
      "ID: 513, Token: rdbms\n",
      "ID: 514, Token: rodbc\n",
      "ID: 515, Token: stored\n",
      "ID: 516, Token: we\n",
      "ID: 517, Token: when\n",
      "ID: 518, Token: building\n",
      "ID: 519, Token: critical\n",
      "ID: 520, Token: exposes\n",
      "ID: 521, Token: field\n",
      "ID: 522, Token: foundational\n",
      "ID: 523, Token: ii\n",
      "ID: 524, Token: knowledge\n",
      "ID: 525, Token: level\n",
      "ID: 526, Token: move\n",
      "ID: 527, Token: next\n",
      "ID: 528, Token: operations\n",
      "ID: 529, Token: opportunity\n",
      "ID: 530, Token: resilient\n",
      "ID: 531, Token: set\n",
      "ID: 532, Token: skills\n",
      "ID: 533, Token: success\n",
      "ID: 534, Token: calculate\n",
      "ID: 535, Token: descriptive\n",
      "ID: 536, Token: insurance\n",
      "ID: 537, Token: statistical\n",
      "ID: 538, Token: statistics\n",
      "ID: 539, Token: apply\n",
      "ID: 540, Token: available\n",
      "ID: 541, Token: catalog\n",
      "ID: 542, Token: mini\n",
      "ID: 543, Token: practice\n",
      "ID: 544, Token: put\n",
      "ID: 545, Token: sharing\n",
      "ID: 546, Token: them\n",
      "ID: 547, Token: ñ\n",
      "ID: 548, Token: advanced\n",
      "ID: 549, Token: associated\n",
      "ID: 550, Token: came\n",
      "ID: 551, Token: class\n",
      "ID: 552, Token: core\n",
      "ID: 553, Token: courses\n",
      "ID: 554, Token: dataframe\n",
      "ID: 555, Token: dataframes\n",
      "ID: 556, Token: deeper\n",
      "ID: 557, Token: ecosystem\n",
      "ID: 558, Token: establish\n",
      "ID: 559, Token: finished\n",
      "ID: 560, Token: general\n",
      "ID: 561, Token: graph\n",
      "ID: 562, Token: having\n",
      "ID: 563, Token: history\n",
      "ID: 564, Token: leverage\n",
      "ID: 565, Token: meant\n",
      "ID: 566, Token: modules\n",
      "ID: 567, Token: other\n",
      "ID: 568, Token: overview\n",
      "ID: 569, Token: prepared\n",
      "ID: 570, Token: rdd\n",
      "ID: 571, Token: rdds\n",
      "ID: 572, Token: recommend\n",
      "ID: 573, Token: student\n",
      "ID: 574, Token: students\n",
      "ID: 575, Token: topics\n",
      "ID: 576, Token: would\n",
      "ID: 577, Token: äù\n",
      "ID: 578, Token: ee\n",
      "ID: 579, Token: jakarta\n",
      "ID: 580, Token: microshed\n",
      "ID: 581, Token: 101\n",
      "ID: 582, Token: 40\n",
      "ID: 583, Token: activities\n",
      "ID: 584, Token: beautiful\n",
      "ID: 585, Token: becoming\n",
      "ID: 586, Token: completing\n",
      "ID: 587, Token: every\n",
      "ID: 588, Token: factors\n",
      "ID: 589, Token: frames\n",
      "ID: 590, Token: free\n",
      "ID: 591, Token: gained\n",
      "ID: 592, Token: grows\n",
      "ID: 593, Token: increasing\n",
      "ID: 594, Token: leading\n",
      "ID: 595, Token: lists\n",
      "ID: 596, Token: million\n",
      "ID: 597, Token: number\n",
      "ID: 598, Token: online\n",
      "ID: 599, Token: organizations\n",
      "ID: 600, Token: own\n",
      "ID: 601, Token: rapidly\n",
      "ID: 602, Token: ready\n",
      "ID: 603, Token: today\n",
      "ID: 604, Token: undertake\n",
      "ID: 605, Token: very\n",
      "ID: 606, Token: worldwide\n",
      "ID: 607, Token: year\n",
      "ID: 608, Token: text\n",
      "ID: 609, Token: better\n",
      "ID: 610, Token: cool\n",
      "ID: 611, Token: expose\n",
      "ID: 612, Token: full\n",
      "ID: 613, Token: make\n",
      "ID: 614, Token: mobile\n",
      "ID: 615, Token: monetize\n",
      "ID: 616, Token: side\n",
      "ID: 617, Token: assessment\n",
      "ID: 618, Token: carlo\n",
      "ID: 619, Token: method\n",
      "ID: 620, Token: monte\n",
      "ID: 621, Token: montecarlo\n",
      "ID: 622, Token: probability\n",
      "ID: 623, Token: risk\n",
      "ID: 624, Token: ruin\n",
      "ID: 625, Token: 201\n",
      "ID: 626, Token: anlysis\n",
      "ID: 627, Token: teaching\n",
      "ID: 628, Token: 301\n",
      "ID: 629, Token: convert\n",
      "ID: 630, Token: emotion\n",
      "ID: 631, Token: ios\n",
      "ID: 632, Token: photo\n",
      "ID: 633, Token: sentiment\n",
      "ID: 634, Token: so\n",
      "ID: 635, Token: speech\n",
      "ID: 636, Token: three\n",
      "ID: 637, Token: various\n",
      "ID: 638, Token: add\n",
      "ID: 639, Token: animal\n",
      "ID: 640, Token: barking\n",
      "ID: 641, Token: cat\n",
      "ID: 642, Token: dog\n",
      "ID: 643, Token: identify\n",
      "ID: 644, Token: image\n",
      "ID: 645, Token: integrate\n",
      "ID: 646, Token: making\n",
      "ID: 647, Token: page\n",
      "ID: 648, Token: purring\n",
      "ID: 649, Token: recognition\n",
      "ID: 650, Token: specific\n",
      "ID: 651, Token: visual\n",
      "ID: 652, Token: communicate\n",
      "ID: 653, Token: fundamental\n",
      "ID: 654, Token: job\n",
      "ID: 655, Token: mapreduce\n",
      "ID: 656, Token: random\n",
      "ID: 657, Token: real\n",
      "ID: 658, Token: writes\n",
      "ID: 659, Token: welcome\n",
      "ID: 660, Token: actuarial\n",
      "ID: 661, Token: business\n",
      "ID: 662, Token: calculations\n",
      "ID: 663, Token: mathematical\n",
      "ID: 664, Token: model\n",
      "ID: 665, Token: modelling\n",
      "ID: 666, Token: process\n",
      "ID: 667, Token: revenue\n",
      "ID: 668, Token: accessing\n",
      "ID: 669, Token: easily\n",
      "ID: 670, Token: hive\n",
      "ID: 671, Token: projects\n",
      "ID: 672, Token: warehousing\n",
      "ID: 673, Token: beyond\n",
      "ID: 674, Token: health\n",
      "ID: 675, Token: istio\n",
      "ID: 676, Token: kubernetes\n",
      "ID: 677, Token: managing\n",
      "ID: 678, Token: mesh\n",
      "ID: 679, Token: observe\n",
      "ID: 680, Token: secure\n",
      "ID: 681, Token: shows\n",
      "ID: 682, Token: start\n",
      "ID: 683, Token: traffic\n",
      "ID: 684, Token: capable\n",
      "ID: 685, Token: capture\n",
      "ID: 686, Token: discovering\n",
      "ID: 687, Token: hidden\n",
      "ID: 688, Token: instance\n",
      "ID: 689, Token: kind\n",
      "ID: 690, Token: library\n",
      "ID: 691, Token: majority\n",
      "ID: 692, Token: relevant\n",
      "ID: 693, Token: shallow\n",
      "ID: 694, Token: solve\n",
      "ID: 695, Token: structure\n",
      "ID: 696, Token: structures\n",
      "ID: 697, Token: unlabeled\n",
      "ID: 698, Token: unstructured\n",
      "ID: 699, Token: applicable\n",
      "ID: 700, Token: being\n",
      "ID: 701, Token: blogs\n",
      "ID: 702, Token: brand\n",
      "ID: 703, Token: call\n",
      "ID: 704, Token: comments\n",
      "ID: 705, Token: competitors\n",
      "ID: 706, Token: constitutes\n",
      "ID: 707, Token: customers\n",
      "ID: 708, Token: emails\n",
      "ID: 709, Token: employees\n",
      "ID: 710, Token: example\n",
      "ID: 711, Token: face\n",
      "ID: 712, Token: find\n",
      "ID: 713, Token: forms\n",
      "ID: 714, Token: forums\n",
      "ID: 715, Token: industries\n",
      "ID: 716, Token: leaked\n",
      "ID: 717, Token: measure\n",
      "ID: 718, Token: millions\n",
      "ID: 719, Token: most\n",
      "ID: 720, Token: negative\n",
      "ID: 721, Token: pain\n",
      "ID: 722, Token: perceptions\n",
      "ID: 723, Token: points\n",
      "ID: 724, Token: positive\n",
      "ID: 725, Token: product\n",
      "ID: 726, Token: products\n",
      "ID: 727, Token: questions\n",
      "ID: 728, Token: secrets\n",
      "ID: 729, Token: suspicious\n",
      "ID: 730, Token: tweets\n",
      "ID: 731, Token: accurate\n",
      "ID: 732, Token: addressed\n",
      "ID: 733, Token: both\n",
      "ID: 734, Token: cascading\n",
      "ID: 735, Token: continuation\n",
      "ID: 736, Token: declarative\n",
      "ID: 737, Token: detail\n",
      "ID: 738, Token: domain\n",
      "ID: 739, Token: early\n",
      "ID: 740, Token: enhance\n",
      "ID: 741, Token: explains\n",
      "ID: 742, Token: expressivity\n",
      "ID: 743, Token: extraction\n",
      "ID: 744, Token: extractors\n",
      "ID: 745, Token: formalism\n",
      "ID: 746, Token: grammars\n",
      "ID: 747, Token: information\n",
      "ID: 748, Token: limitations\n",
      "ID: 749, Token: maintain\n",
      "ID: 750, Token: new\n",
      "ID: 751, Token: principles\n",
      "ID: 752, Token: resulting\n",
      "ID: 753, Token: results\n",
      "ID: 754, Token: runtime\n",
      "ID: 755, Token: scalable\n",
      "ID: 756, Token: standard\n",
      "ID: 757, Token: suffer\n",
      "ID: 758, Token: systemt\n",
      "ID: 759, Token: via\n",
      "ID: 760, Token: algebra\n",
      "ID: 761, Token: automatic\n",
      "ID: 762, Token: constructs\n",
      "ID: 763, Token: expressed\n",
      "ID: 764, Token: generation\n",
      "ID: 765, Token: includes\n",
      "ID: 766, Token: linear\n",
      "ID: 767, Token: ml\n",
      "ID: 768, Token: optimized\n",
      "ID: 769, Token: plans\n",
      "ID: 770, Token: primitives\n",
      "ID: 771, Token: ranging\n",
      "ID: 772, Token: single\n",
      "ID: 773, Token: style\n",
      "ID: 774, Token: systemml\n",
      "ID: 775, Token: action\n",
      "ID: 776, Token: firewall\n",
      "ID: 777, Token: internet\n",
      "ID: 778, Token: logs\n",
      "ID: 779, Token: task\n",
      "ID: 780, Token: orchestration\n",
      "ID: 781, Token: 19\n",
      "ID: 782, Token: care\n",
      "ID: 783, Token: covid\n",
      "ID: 784, Token: dynamics\n",
      "ID: 785, Token: prognostication\n",
      "ID: 786, Token: spread\n",
      "ID: 787, Token: areas\n",
      "ID: 788, Token: collection\n",
      "ID: 789, Token: frameworks\n",
      "ID: 790, Token: helping\n",
      "ID: 791, Token: junction\n",
      "ID: 792, Token: nlp\n",
      "ID: 793, Token: sms\n",
      "ID: 794, Token: spam\n",
      "ID: 795, Token: banking\n",
      "ID: 796, Token: area\n",
      "ID: 797, Token: card\n",
      "ID: 798, Token: clients\n",
      "ID: 799, Token: credit\n",
      "ID: 800, Token: preliminary\n",
      "ID: 801, Token: anomaly\n",
      "ID: 802, Token: detection\n",
      "ID: 803, Token: evaluation\n",
      "ID: 804, Token: intrusion\n",
      "ID: 805, Token: iscxids2012\n",
      "ID: 806, Token: backend\n",
      "ID: 807, Token: because\n",
      "ID: 808, Token: before\n",
      "ID: 809, Token: frontend\n",
      "ID: 810, Token: heard\n",
      "ID: 811, Token: js\n",
      "ID: 812, Token: know\n",
      "ID: 813, Token: technology\n",
      "ID: 814, Token: why\n",
      "ID: 815, Token: backup\n",
      "ID: 816, Token: cli\n",
      "ID: 817, Token: command\n",
      "ID: 818, Token: contents\n",
      "ID: 819, Token: dump\n",
      "ID: 820, Token: finally\n",
      "ID: 821, Token: restore\n",
      "ID: 822, Token: tables\n",
      "ID: 823, Token: excited\n",
      "ID: 824, Token: features\n",
      "ID: 825, Token: functionality\n",
      "ID: 826, Token: implement\n",
      "ID: 827, Token: satellite\n",
      "ID: 828, Token: quantum\n",
      "ID: 829, Token: strange\n",
      "ID: 830, Token: all\n",
      "ID: 831, Token: app\n",
      "ID: 832, Token: debate\n",
      "ID: 833, Token: detector\n",
      "ID: 834, Token: dollar\n",
      "ID: 835, Token: ever\n",
      "ID: 836, Token: great\n",
      "ID: 837, Token: hotdog\n",
      "ID: 838, Token: idea\n",
      "ID: 839, Token: land\n",
      "ID: 840, Token: launch\n",
      "ID: 841, Token: let\n",
      "ID: 842, Token: longer\n",
      "ID: 843, Token: now\n",
      "ID: 844, Token: once\n",
      "ID: 845, Token: picture\n",
      "ID: 846, Token: prove\n",
      "ID: 847, Token: pursue\n",
      "ID: 848, Token: settle\n",
      "ID: 849, Token: tells\n",
      "ID: 850, Token: whether\n",
      "ID: 851, Token: wonder\n",
      "ID: 852, Token: wondered\n",
      "ID: 853, Token: eda\n",
      "ID: 854, Token: exploratory\n",
      "ID: 855, Token: pandas\n",
      "ID: 856, Token: articles\n",
      "ID: 857, Token: derive\n",
      "ID: 858, Token: enroll\n",
      "ID: 859, Token: everywhere\n",
      "ID: 860, Token: experts\n",
      "ID: 861, Token: hot\n",
      "ID: 862, Token: insights\n",
      "ID: 863, Token: interested\n",
      "ID: 864, Token: news\n",
      "ID: 865, Token: valuable\n",
      "ID: 866, Token: graphical\n",
      "ID: 867, Token: gui\n",
      "ID: 868, Token: phpmyadmin\n",
      "ID: 869, Token: attribute\n",
      "ID: 870, Token: cardinality\n",
      "ID: 871, Token: degree\n",
      "ID: 872, Token: entity\n",
      "ID: 873, Token: learned\n",
      "ID: 874, Token: relation\n",
      "ID: 875, Token: terms\n",
      "ID: 876, Token: achievement\n",
      "ID: 877, Token: after\n",
      "ID: 878, Token: certificate\n",
      "ID: 879, Token: completed\n",
      "ID: 880, Token: download\n",
      "ID: 881, Token: labs\n",
      "ID: 882, Token: many\n",
      "ID: 883, Token: modeler\n",
      "ID: 884, Token: modeling\n",
      "ID: 885, Token: only\n",
      "ID: 886, Token: predictive\n",
      "ID: 887, Token: print\n",
      "ID: 888, Token: spss\n",
      "ID: 889, Token: they\n",
      "ID: 890, Token: totally\n",
      "ID: 891, Token: trial\n",
      "ID: 892, Token: where\n",
      "ID: 893, Token: wish\n",
      "ID: 894, Token: asset\n",
      "ID: 895, Token: blockchain\n",
      "ID: 896, Token: chain\n",
      "ID: 897, Token: configure\n",
      "ID: 898, Token: dashboard\n",
      "ID: 899, Token: device\n",
      "ID: 900, Token: don\n",
      "ID: 901, Token: monitor\n",
      "ID: 902, Token: perishable\n",
      "ID: 903, Token: purchase\n",
      "ID: 904, Token: simulated\n",
      "ID: 905, Token: supply\n",
      "ID: 906, Token: t\n",
      "ID: 907, Token: tracker\n",
      "ID: 908, Token: tracking\n",
      "ID: 909, Token: chaincode\n",
      "ID: 910, Token: developing\n",
      "ID: 911, Token: postgresql\n",
      "ID: 912, Token: components\n",
      "ID: 913, Token: composer\n",
      "ID: 914, Token: consensus\n",
      "ID: 915, Token: contracts\n",
      "ID: 916, Token: foundation\n",
      "ID: 917, Token: hyperledger\n",
      "ID: 918, Token: ledgers\n",
      "ID: 919, Token: between\n",
      "ID: 920, Token: enforce\n",
      "ID: 921, Token: entry\n",
      "ID: 922, Token: keys\n",
      "ID: 923, Token: relationships\n",
      "ID: 924, Token: rules\n",
      "ID: 925, Token: addition\n",
      "ID: 926, Token: allows\n",
      "ID: 927, Token: builders\n",
      "ID: 928, Token: computation\n",
      "ID: 929, Token: edges\n",
      "ID: 930, Token: exploring\n",
      "ID: 931, Token: graphx\n",
      "ID: 932, Token: growing\n",
      "ID: 933, Token: operators\n",
      "ID: 934, Token: paradigm\n",
      "ID: 935, Token: parallel\n",
      "ID: 936, Token: representation\n",
      "ID: 937, Token: simplify\n",
      "ID: 938, Token: tasks\n",
      "ID: 939, Token: vertices\n",
      "ID: 940, Token: blood\n",
      "ID: 941, Token: collected\n",
      "ID: 942, Token: diseases\n",
      "ID: 943, Token: elisa\n",
      "ID: 944, Token: groups\n",
      "ID: 945, Token: igg\n",
      "ID: 946, Token: igm\n",
      "ID: 947, Token: influenza\n",
      "ID: 948, Token: pre\n",
      "ID: 949, Token: preparate\n",
      "ID: 950, Token: previous\n",
      "ID: 951, Token: tuberculosis\n",
      "ID: 952, Token: vaccination\n",
      "ID: 953, Token: pgadmin\n",
      "ID: 954, Token: metrics\n",
      "ID: 955, Token: monitoring\n",
      "ID: 956, Token: config\n",
      "ID: 957, Token: configmaps\n",
      "ID: 958, Token: configuration\n",
      "ID: 959, Token: configuring\n",
      "ID: 960, Token: externalize\n",
      "ID: 961, Token: 2048\n",
      "ID: 962, Token: agents\n",
      "ID: 963, Token: cartpole\n",
      "ID: 964, Token: games\n",
      "ID: 965, Token: going\n",
      "ID: 966, Token: play\n",
      "ID: 967, Token: playing\n",
      "ID: 968, Token: s4tf\n",
      "ID: 969, Token: tac\n",
      "ID: 970, Token: tic\n",
      "ID: 971, Token: toe\n",
      "ID: 972, Token: code\n",
      "ID: 973, Token: cors\n",
      "ID: 974, Token: cross\n",
      "ID: 975, Token: origin\n",
      "ID: 976, Token: resource\n",
      "ID: 977, Token: writing\n",
      "ID: 978, Token: actual\n",
      "ID: 979, Token: allow\n",
      "ID: 980, Token: almost\n",
      "ID: 981, Token: anything\n",
      "ID: 982, Token: developed\n",
      "ID: 983, Token: eat\n",
      "ID: 984, Token: hence\n",
      "ID: 985, Token: initially\n",
      "ID: 986, Token: less\n",
      "ID: 987, Token: mapper\n",
      "ID: 988, Token: name\n",
      "ID: 989, Token: people\n",
      "ID: 990, Token: pig\n",
      "ID: 991, Token: pigs\n",
      "ID: 992, Token: programs\n",
      "ID: 993, Token: reducer\n",
      "ID: 994, Token: sets\n",
      "ID: 995, Token: spend\n",
      "ID: 996, Token: was\n",
      "ID: 997, Token: who\n",
      "ID: 998, Token: write\n",
      "ID: 999, Token: yahoo\n",
      "ID: 1000, Token: biginsights\n",
      "ID: 1001, Token: contextualize\n",
      "ID: 1002, Token: examples\n",
      "ID: 1003, Token: manage\n",
      "ID: 1004, Token: motivate\n",
      "ID: 1005, Token: study\n",
      "ID: 1006, Token: tandem\n",
      "ID: 1007, Token: zookeeper\n",
      "ID: 1008, Token: controlling\n",
      "ID: 1009, Token: even\n",
      "ID: 1010, Token: jobs\n",
      "ID: 1011, Token: materials\n",
      "ID: 1012, Token: oozie\n",
      "ID: 1013, Token: provided\n",
      "ID: 1014, Token: click\n",
      "ID: 1015, Token: commands\n",
      "ID: 1016, Token: covered\n",
      "ID: 1017, Token: describes\n",
      "ID: 1018, Token: flume\n",
      "ID: 1019, Token: greater\n",
      "ID: 1020, Token: moving\n",
      "ID: 1021, Token: presented\n",
      "ID: 1022, Token: shell\n",
      "ID: 1023, Token: sophisticated\n",
      "ID: 1024, Token: sqoop\n",
      "ID: 1025, Token: techniques\n",
      "ID: 1026, Token: variety\n",
      "ID: 1027, Token: ways\n",
      "ID: 1028, Token: exposure\n",
      "ID: 1029, Token: gaining\n",
      "ID: 1030, Token: mapreduce1\n",
      "ID: 1031, Token: negotiator\n",
      "ID: 1032, Token: string\n",
      "ID: 1033, Token: yarn\n",
      "ID: 1034, Token: yet\n",
      "ID: 1035, Token: acknowledge\n",
      "ID: 1036, Token: acknowledging\n",
      "ID: 1037, Token: messages\n",
      "ID: 1038, Token: messaging\n",
      "ID: 1039, Token: always\n",
      "ID: 1040, Token: choosing\n",
      "ID: 1041, Token: decision\n",
      "ID: 1042, Token: django\n",
      "ID: 1043, Token: evening\n",
      "ID: 1044, Token: handy\n",
      "ID: 1045, Token: hard\n",
      "ID: 1046, Token: movie\n",
      "ID: 1047, Token: personal\n",
      "ID: 1048, Token: recommender\n",
      "ID: 1049, Token: watch\n",
      "ID: 1050, Token: weekend\n",
      "ID: 1051, Token: cdi\n",
      "ID: 1052, Token: contexts\n",
      "ID: 1053, Token: dependency\n",
      "ID: 1054, Token: inject\n",
      "ID: 1055, Token: injecting\n",
      "ID: 1056, Token: injection\n",
      "ID: 1057, Token: scopes\n",
      "ID: 1058, Token: essential\n",
      "ID: 1059, Token: ignite\n",
      "ID: 1060, Token: interest\n",
      "ID: 1061, Token: processor\n",
      "ID: 1062, Token: deploying\n",
      "ID: 1063, Token: kubectl\n",
      "ID: 1064, Token: maven\n",
      "ID: 1065, Token: rest\n",
      "ID: 1066, Token: assistant\n",
      "ID: 1067, Token: brings\n",
      "ID: 1068, Token: challenging\n",
      "ID: 1069, Token: correlation\n",
      "ID: 1070, Token: ends\n",
      "ID: 1071, Token: evaluations\n",
      "ID: 1072, Token: exercise\n",
      "ID: 1073, Token: fail\n",
      "ID: 1074, Token: fully\n",
      "ID: 1075, Token: gentle\n",
      "ID: 1076, Token: haider\n",
      "ID: 1077, Token: included\n",
      "ID: 1078, Token: looking\n",
      "ID: 1079, Token: murtaza\n",
      "ID: 1080, Token: professor\n",
      "ID: 1081, Token: ryerson\n",
      "ID: 1082, Token: taught\n",
      "ID: 1083, Token: variance\n",
      "ID: 1084, Token: visualization\n",
      "ID: 1085, Token: won\n",
      "ID: 1086, Token: ability\n",
      "ID: 1087, Token: able\n",
      "ID: 1088, Token: analyst\n",
      "ID: 1089, Token: bigsheets\n",
      "ID: 1090, Token: capabilities\n",
      "ID: 1091, Token: component\n",
      "ID: 1092, Token: spreadsheet\n",
      "ID: 1093, Token: type\n",
      "ID: 1094, Token: ui\n",
      "ID: 1095, Token: visualize\n",
      "ID: 1096, Token: consultant\n",
      "ID: 1097, Token: openrefine\n",
      "ID: 1098, Token: technical\n",
      "ID: 1099, Token: solr\n",
      "ID: 1100, Token: annotation\n",
      "ID: 1101, Token: behaviours\n",
      "ID: 1102, Token: failures\n",
      "ID: 1103, Token: fallback\n",
      "ID: 1104, Token: fault\n",
      "ID: 1105, Token: impact\n",
      "ID: 1106, Token: retry\n",
      "ID: 1107, Token: tolerance\n",
      "ID: 1108, Token: tolerant\n",
      "ID: 1109, Token: interfaces\n",
      "ID: 1110, Token: safe\n",
      "ID: 1111, Token: template\n",
      "ID: 1112, Token: chatbot\n",
      "ID: 1113, Token: chatbots\n",
      "ID: 1114, Token: complete\n",
      "ID: 1115, Token: conversation\n",
      "ID: 1116, Token: conversational\n",
      "ID: 1117, Token: culminating\n",
      "ID: 1118, Token: depth\n",
      "ID: 1119, Token: functional\n",
      "ID: 1120, Token: seven\n",
      "ID: 1121, Token: bots\n",
      "ID: 1122, Token: cognitive\n",
      "ID: 1123, Token: facebook\n",
      "ID: 1124, Token: formerly\n",
      "ID: 1125, Token: messenger\n",
      "ID: 1126, Token: send\n",
      "ID: 1127, Token: tone\n",
      "ID: 1128, Token: translate\n",
      "ID: 1129, Token: inconsistency\n",
      "ID: 1130, Token: integrity\n",
      "ID: 1131, Token: kinds\n",
      "ID: 1132, Token: lastly\n",
      "ID: 1133, Token: minimize\n",
      "ID: 1134, Token: normalization\n",
      "ID: 1135, Token: normalizing\n",
      "ID: 1136, Token: record\n",
      "ID: 1137, Token: redundancy\n",
      "ID: 1138, Token: relationship\n",
      "ID: 1139, Token: uniquely\n",
      "ID: 1140, Token: agile\n",
      "ID: 1141, Token: best\n",
      "ID: 1142, Token: changed\n",
      "ID: 1143, Token: driven\n",
      "ID: 1144, Token: foundry\n",
      "ID: 1145, Token: practices\n",
      "ID: 1146, Token: discover\n",
      "ID: 1147, Token: pair\n",
      "ID: 1148, Token: securely\n",
      "ID: 1149, Token: agent\n",
      "ID: 1150, Token: called\n",
      "ID: 1151, Token: difference\n",
      "ID: 1152, Token: gym\n",
      "ID: 1153, Token: loses\n",
      "ID: 1154, Token: never\n",
      "ID: 1155, Token: openai\n",
      "ID: 1156, Token: reinforcement\n",
      "ID: 1157, Token: temporal\n",
      "ID: 1158, Token: tictactoe\n",
      "ID: 1159, Token: hat\n",
      "ID: 1160, Token: openshift\n",
      "ID: 1161, Token: operator\n",
      "ID: 1162, Token: demonstrations\n",
      "ID: 1163, Token: demos\n",
      "ID: 1164, Token: goal\n",
      "ID: 1165, Token: inspiring\n",
      "ID: 1166, Token: providing\n",
      "ID: 1167, Token: showing\n",
      "ID: 1168, Token: answers\n",
      "ID: 1169, Token: approach\n",
      "ID: 1170, Token: holistic\n",
      "ID: 1171, Token: journalism\n",
      "ID: 1172, Token: presents\n",
      "ID: 1173, Token: steps\n",
      "ID: 1174, Token: optimization\n",
      "ID: 1175, Token: academic\n",
      "ID: 1176, Token: additionally\n",
      "ID: 1177, Token: bind\n",
      "ID: 1178, Token: created\n",
      "ID: 1179, Token: custom\n",
      "ID: 1180, Token: locally\n",
      "ID: 1181, Token: scaffold\n",
      "ID: 1182, Token: files\n",
      "ID: 1183, Token: ideal\n",
      "ID: 1184, Token: powerful\n",
      "ID: 1185, Token: account\n",
      "ID: 1186, Token: additional\n",
      "ID: 1187, Token: benefits\n",
      "ID: 1188, Token: cloudant\n",
      "ID: 1189, Token: conclude\n",
      "ID: 1190, Token: continue\n",
      "ID: 1191, Token: dbaas\n",
      "ID: 1192, Token: education\n",
      "ID: 1193, Token: fits\n",
      "ID: 1194, Token: landscape\n",
      "ID: 1195, Token: loading\n",
      "ID: 1196, Token: nosql\n",
      "ID: 1197, Token: offering\n",
      "ID: 1198, Token: pointing\n",
      "ID: 1199, Token: querying\n",
      "ID: 1200, Token: replicating\n",
      "ID: 1201, Token: resources\n",
      "ID: 1202, Token: sign\n",
      "ID: 1203, Token: tutorials\n",
      "ID: 1204, Token: covers\n",
      "ID: 1205, Token: lifecycle\n",
      "ID: 1206, Token: manager\n",
      "ID: 1207, Token: reconciliation\n",
      "ID: 1208, Token: scorecard\n",
      "ID: 1209, Token: upgrade\n",
      "ID: 1210, Token: knative\n",
      "ID: 1211, Token: powered\n",
      "ID: 1212, Token: career\n",
      "ID: 1213, Token: talks\n",
      "ID: 1214, Token: bitcoin\n",
      "ID: 1215, Token: greetings\n",
      "ID: 1216, Token: ide\n",
      "ID: 1217, Token: jupyter\n",
      "ID: 1218, Token: notebooks\n",
      "ID: 1219, Token: rstudio\n",
      "ID: 1220, Token: scientists\n",
      "ID: 1221, Token: zeppelin\n",
      "ID: 1222, Token: beakers\n",
      "ID: 1223, Token: calculator\n",
      "ID: 1224, Token: coat\n",
      "ID: 1225, Token: determining\n",
      "ID: 1226, Token: emerging\n",
      "ID: 1227, Token: forward\n",
      "ID: 1228, Token: grab\n",
      "ID: 1229, Token: methodologies\n",
      "ID: 1230, Token: passed\n",
      "ID: 1231, Token: path\n",
      "ID: 1232, Token: pocket\n",
      "ID: 1233, Token: rather\n",
      "ID: 1234, Token: wait\n",
      "ID: 1235, Token: wave\n",
      "ID: 1236, Token: waves\n",
      "ID: 1237, Token: wrong\n",
      "ID: 1238, Token: asynchronous\n",
      "ID: 1239, Token: algolia\n",
      "ID: 1240, Token: feature\n",
      "ID: 1241, Token: form\n",
      "ID: 1242, Token: scratch\n",
      "ID: 1243, Token: search\n",
      "ID: 1244, Token: share\n",
      "ID: 1245, Token: documenting\n",
      "ID: 1246, Token: filter\n",
      "ID: 1247, Token: openapi\n",
      "ID: 1248, Token: static\n",
      "ID: 1249, Token: patterns\n",
      "ID: 1250, Token: agriculture\n",
      "ID: 1251, Token: transformation\n",
      "ID: 1252, Token: cqrs\n",
      "ID: 1253, Token: event\n",
      "ID: 1254, Token: sourcing\n",
      "ID: 1255, Token: monoliths\n",
      "ID: 1256, Token: talking\n",
      "ID: 1257, Token: term\n",
      "ID: 1258, Token: usually\n",
      "ID: 1259, Token: commonly\n",
      "ID: 1260, Token: design\n",
      "ID: 1261, Token: elements\n",
      "ID: 1262, Token: relate\n",
      "ID: 1263, Token: technique\n",
      "ID: 1264, Token: arm\n",
      "ID: 1265, Token: flash\n",
      "ID: 1266, Token: instead\n",
      "ID: 1267, Token: light\n",
      "ID: 1268, Token: simulator\n",
      "ID: 1269, Token: speak\n",
      "ID: 1270, Token: tjbot\n",
      "ID: 1271, Token: administrators\n",
      "ID: 1272, Token: anyone\n",
      "ID: 1273, Token: decisions\n",
      "ID: 1274, Token: delivering\n",
      "ID: 1275, Token: devops\n",
      "ID: 1276, Token: engineering\n",
      "ID: 1277, Token: enterprise\n",
      "ID: 1278, Token: environments\n",
      "ID: 1279, Token: informed\n",
      "ID: 1280, Token: legacy\n",
      "ID: 1281, Token: migrating\n",
      "ID: 1282, Token: migration\n",
      "ID: 1283, Token: modernization\n",
      "ID: 1284, Token: modernizing\n",
      "ID: 1285, Token: multicloud\n",
      "ID: 1286, Token: related\n",
      "ID: 1287, Token: wants\n",
      "ID: 1288, Token: discussions\n",
      "ID: 1289, Token: does\n",
      "ID: 1290, Token: matter\n",
      "ID: 1291, Token: terminology\n",
      "ID: 1292, Token: badge\n",
      "ID: 1293, Token: earn\n",
      "ID: 1294, Token: options\n",
      "ID: 1295, Token: platforms\n",
      "ID: 1296, Token: forecasting\n",
      "ID: 1297, Token: regression\n",
      "ID: 1298, Token: availability\n",
      "ID: 1299, Token: discovery\n",
      "ID: 1300, Token: high\n",
      "ID: 1301, Token: production\n",
      "ID: 1302, Token: gpus\n",
      "ID: 1303, Token: five\n",
      "ID: 1304, Token: leveraging\n",
      "ID: 1305, Token: minutes\n",
      "ID: 1306, Token: site\n",
      "ID: 1307, Token: display\n",
      "ID: 1308, Token: dynamic\n",
      "ID: 1309, Token: geo\n",
      "ID: 1310, Token: maps\n",
      "ID: 1311, Token: visualisation\n",
      "ID: 1312, Token: again\n",
      "ID: 1313, Token: ball\n",
      "ID: 1314, Token: beneficial\n",
      "ID: 1315, Token: bit\n",
      "ID: 1316, Token: blue\n",
      "ID: 1317, Token: certain\n",
      "ID: 1318, Token: die\n",
      "ID: 1319, Token: eight\n",
      "ID: 1320, Token: future\n",
      "ID: 1321, Token: hazy\n",
      "ID: 1322, Token: indicator\n",
      "ID: 1323, Token: oil\n",
      "ID: 1324, Token: phrases\n",
      "ID: 1325, Token: point\n",
      "ID: 1326, Token: predictions\n",
      "ID: 1327, Token: rely\n",
      "ID: 1328, Token: reply\n",
      "ID: 1329, Token: room\n",
      "ID: 1330, Token: shelf\n",
      "ID: 1331, Token: signs\n",
      "ID: 1332, Token: trends\n",
      "ID: 1333, Token: try\n",
      "ID: 1334, Token: yes\n",
      "ID: 1335, Token: dimensionality\n",
      "ID: 1336, Token: reduction\n",
      "ID: 1337, Token: give\n",
      "ID: 1338, Token: incredibly\n",
      "ID: 1339, Token: supervised\n",
      "ID: 1340, Token: uncover\n",
      "ID: 1341, Token: unsupervised\n",
      "ID: 1342, Token: b\n",
      "ID: 1343, Token: consume\n",
      "ID: 1344, Token: p\n",
      "ID: 1345, Token: challenges\n",
      "ID: 1346, Token: issues\n",
      "ID: 1347, Token: market\n",
      "ID: 1348, Token: segment\n",
      "ID: 1349, Token: target\n",
      "ID: 1350, Token: acquisition\n",
      "ID: 1351, Token: insight\n",
      "ID: 1352, Token: obtain\n",
      "ID: 1353, Token: check\n",
      "ID: 1354, Token: checking\n",
      "ID: 1355, Token: endpoints\n",
      "ID: 1356, Token: inspect\n",
      "ID: 1357, Token: liveness\n",
      "ID: 1358, Token: probes\n",
      "ID: 1359, Token: readiness\n",
      "ID: 1360, Token: setting\n",
      "ID: 1361, Token: appreciation\n",
      "ID: 1362, Token: assumed\n",
      "ID: 1363, Token: confidently\n",
      "ID: 1364, Token: could\n",
      "ID: 1365, Token: curricula\n",
      "ID: 1366, Token: developers\n",
      "ID: 1367, Token: ensures\n",
      "ID: 1368, Token: experienced\n",
      "ID: 1369, Token: refresher\n",
      "ID: 1370, Token: solid\n",
      "ID: 1371, Token: tooling\n",
      "ID: 1372, Token: typesafe\n",
      "ID: 1373, Token: useful\n",
      "ID: 1374, Token: report\n",
      "ID: 1375, Token: bird\n",
      "ID: 1376, Token: days\n",
      "ID: 1377, Token: detect\n",
      "ID: 1378, Token: did\n",
      "ID: 1379, Token: lot\n",
      "ID: 1380, Token: machines\n",
      "ID: 1381, Token: object\n",
      "ID: 1382, Token: plane\n",
      "ID: 1383, Token: pytorch\n",
      "ID: 1384, Token: recognizing\n",
      "ID: 1385, Token: repeated\n",
      "ID: 1386, Token: superman\n",
      "ID: 1387, Token: train\n",
      "ID: 1388, Token: external\n",
      "ID: 1389, Token: externalizing\n",
      "ID: 1390, Token: integrating\n",
      "ID: 1391, Token: cql\n",
      "ID: 1392, Token: execute\n",
      "ID: 1393, Token: keyspace\n",
      "ID: 1394, Token: cqlsh\n",
      "ID: 1395, Token: crud\n",
      "ID: 1396, Token: delete\n",
      "ID: 1397, Token: assessments\n",
      "ID: 1398, Token: instructions\n",
      "ID: 1399, Token: preview\n",
      "ID: 1400, Token: publish\n",
      "ID: 1401, Token: version\n",
      "ID: 1402, Token: weeks\n",
      "ID: 1403, Token: artistic\n",
      "ID: 1404, Token: creativity\n",
      "ID: 1405, Token: opencv\n",
      "ID: 1406, Token: painting\n",
      "ID: 1407, Token: paintings\n",
      "ID: 1408, Token: photographs\n",
      "ID: 1409, Token: photos\n",
      "ID: 1410, Token: showcase\n",
      "ID: 1411, Token: sketch\n",
      "ID: 1412, Token: sketches\n",
      "ID: 1413, Token: transform\n",
      "ID: 1414, Token: wanted\n",
      "ID: 1415, Token: allocated\n",
      "ID: 1416, Token: compare\n",
      "ID: 1417, Token: decide\n",
      "ID: 1418, Token: experiment\n",
      "ID: 1419, Token: experimental\n",
      "ID: 1420, Token: quick\n",
      "ID: 1421, Token: refers\n",
      "ID: 1422, Token: something\n",
      "ID: 1423, Token: two\n",
      "ID: 1424, Token: versions\n",
      "ID: 1425, Token: accurately\n",
      "ID: 1426, Token: ancient\n",
      "ID: 1427, Token: art\n",
      "ID: 1428, Token: carved\n",
      "ID: 1429, Token: census\n",
      "ID: 1430, Token: distinct\n",
      "ID: 1431, Token: efficiency\n",
      "ID: 1432, Token: egyptians\n",
      "ID: 1433, Token: flooding\n",
      "ID: 1434, Token: increase\n",
      "ID: 1435, Token: meet\n",
      "ID: 1436, Token: nile\n",
      "ID: 1437, Token: out\n",
      "ID: 1438, Token: practitioners\n",
      "ID: 1439, Token: predicted\n",
      "ID: 1440, Token: river\n",
      "ID: 1441, Token: since\n",
      "ID: 1442, Token: tax\n",
      "ID: 1443, Token: uncovering\n",
      "ID: 1444, Token: unique\n",
      "ID: 1445, Token: architectures\n",
      "ID: 1446, Token: behind\n",
      "ID: 1447, Token: contrasts\n",
      "ID: 1448, Token: trying\n",
      "ID: 1449, Token: privacy\n",
      "ID: 1450, Token: angularjs\n",
      "ID: 1451, Token: consumer\n",
      "ID: 1452, Token: contract\n",
      "ID: 1453, Token: pact\n",
      "ID: 1454, Token: gain\n",
      "ID: 1455, Token: perspectives\n",
      "ID: 1456, Token: practitioner\n",
      "ID: 1457, Token: required\n",
      "ID: 1458, Token: become\n",
      "ID: 1459, Token: businesses\n",
      "ID: 1460, Token: divide\n",
      "ID: 1461, Token: extract\n",
      "ID: 1462, Token: introduced\n",
      "ID: 1463, Token: meaning\n",
      "ID: 1464, Token: natural\n",
      "ID: 1465, Token: reviews\n",
      "ID: 1466, Token: subjective\n",
      "ID: 1467, Token: yelp\n",
      "ID: 1468, Token: collaborative\n",
      "ID: 1469, Token: featurization\n",
      "ID: 1470, Token: filtering\n",
      "ID: 1471, Token: handing\n",
      "ID: 1472, Token: handling\n",
      "ID: 1473, Token: known\n",
      "ID: 1474, Token: mllib\n",
      "ID: 1475, Token: persistence\n",
      "ID: 1476, Token: utilities\n",
      "ID: 1477, Token: enterprises\n",
      "ID: 1478, Token: offerings\n",
      "ID: 1479, Token: v3\n",
      "ID: 1480, Token: copying\n",
      "ID: 1481, Token: estate\n",
      "ID: 1482, Token: beginner\n",
      "ID: 1483, Token: friendly\n",
      "ID: 1484, Token: hours\n",
      "ID: 1485, Token: scripts\n",
      "ID: 1486, Token: zero\n",
      "ID: 1487, Token: differ\n",
      "ID: 1488, Token: importance\n",
      "ID: 1489, Token: technologies\n",
      "ID: 1490, Token: v2\n",
      "ID: 1491, Token: brewing\n",
      "ID: 1492, Token: broker\n",
      "ID: 1493, Token: category\n",
      "ID: 1494, Token: connected\n",
      "ID: 1495, Token: describe\n",
      "ID: 1496, Token: hear\n",
      "ID: 1497, Token: kafka\n",
      "ID: 1498, Token: means\n",
      "ID: 1499, Token: producer\n",
      "ID: 1500, Token: simplifying\n",
      "ID: 1501, Token: aspects\n",
      "ID: 1502, Token: discuss\n",
      "ID: 1503, Token: few\n",
      "ID: 1504, Token: guide\n",
      "ID: 1505, Token: appointment\n",
      "ID: 1506, Token: medical\n",
      "ID: 1507, Token: notebook\n",
      "ID: 1508, Token: patients\n",
      "ID: 1509, Token: reasons\n",
      "ID: 1510, Token: charts\n",
      "ID: 1511, Token: customizing\n",
      "ID: 1512, Token: facilitates\n",
      "ID: 1513, Token: findings\n",
      "ID: 1514, Token: graphics\n",
      "ID: 1515, Token: identifying\n",
      "ID: 1516, Token: interpretation\n",
      "ID: 1517, Token: look\n",
      "ID: 1518, Token: presentation\n",
      "ID: 1519, Token: summarize\n",
      "ID: 1520, Token: graphiql\n",
      "ID: 1521, Token: graphql\n",
      "ID: 1522, Token: implementing\n",
      "ID: 1523, Token: multiple\n",
      "ID: 1524, Token: mutations\n",
      "ID: 1525, Token: queries\n",
      "ID: 1526, Token: reactjs\n",
      "ID: 1527, Token: drive\n",
      "ID: 1528, Token: hateoas\n",
      "ID: 1529, Token: hypermedia\n",
      "ID: 1530, Token: state\n",
      "ID: 1531, Token: audience\n",
      "ID: 1532, Token: convey\n",
      "ID: 1533, Token: effective\n",
      "ID: 1534, Token: efficiently\n",
      "ID: 1535, Token: interactively\n",
      "ID: 1536, Token: interesting\n",
      "ID: 1537, Token: order\n",
      "ID: 1538, Token: pleasing\n",
      "ID: 1539, Token: stakeholders\n",
      "ID: 1540, Token: address\n",
      "ID: 1541, Token: arriving\n",
      "ID: 1542, Token: designing\n",
      "ID: 1543, Token: medium\n",
      "ID: 1544, Token: objective\n",
      "ID: 1545, Token: small\n",
      "ID: 1546, Token: ultimately\n",
      "ID: 1547, Token: uses\n",
      "ID: 1548, Token: approachable\n",
      "ID: 1549, Token: comparison\n",
      "ID: 1550, Token: dives\n",
      "ID: 1551, Token: each\n",
      "ID: 1552, Token: relates\n",
      "ID: 1553, Token: vs\n",
      "ID: 1554, Token: head\n",
      "ID: 1555, Token: import\n",
      "ID: 1556, Token: jpa\n",
      "ID: 1557, Token: persist\n",
      "ID: 1558, Token: persisting\n",
      "ID: 1559, Token: virus\n",
      "ID: 1560, Token: jaeger\n",
      "ID: 1561, Token: cd\n",
      "ID: 1562, Token: ci\n",
      "ID: 1563, Token: bot\n",
      "ID: 1564, Token: community\n",
      "ID: 1565, Token: computer\n",
      "ID: 1566, Token: discord\n",
      "ID: 1567, Token: fantastic\n",
      "ID: 1568, Token: friends\n",
      "ID: 1569, Token: host\n",
      "ID: 1570, Token: perfect\n",
      "ID: 1571, Token: somewhere\n",
      "ID: 1572, Token: voice\n",
      "ID: 1573, Token: behave\n",
      "ID: 1574, Token: compared\n",
      "ID: 1575, Token: differently\n",
      "ID: 1576, Token: insert\n",
      "ID: 1577, Token: materialized\n",
      "ID: 1578, Token: regular\n",
      "ID: 1579, Token: result\n",
      "ID: 1580, Token: rows\n",
      "ID: 1581, Token: saved\n",
      "ID: 1582, Token: views\n",
      "ID: 1583, Token: events\n",
      "ID: 1584, Token: front\n",
      "ID: 1585, Token: sent\n",
      "ID: 1586, Token: sse\n",
      "ID: 1587, Token: stream\n",
      "ID: 1588, Token: updates\n",
      "ID: 1589, Token: angular\n",
      "ID: 1590, Token: fit\n",
      "ID: 1591, Token: hyperparameters\n",
      "ID: 1592, Token: optimal\n",
      "ID: 1593, Token: according\n",
      "ID: 1594, Token: analytical\n",
      "ID: 1595, Token: companies\n",
      "ID: 1596, Token: documents\n",
      "ID: 1597, Token: languages\n",
      "ID: 1598, Token: learner\n",
      "ID: 1599, Token: mac\n",
      "ID: 1600, Token: maintainable\n",
      "ID: 1601, Token: operating\n",
      "ID: 1602, Token: reliable\n",
      "ID: 1603, Token: requirements\n",
      "ID: 1604, Token: six\n",
      "ID: 1605, Token: specialization\n",
      "ID: 1606, Token: transactional\n",
      "ID: 1607, Token: virtualbox\n",
      "ID: 1608, Token: vmware\n",
      "ID: 1609, Token: workbench\n",
      "ID: 1610, Token: volume\n",
      "ID: 1611, Token: warehouses\n",
      "ID: 1612, Token: aggregates\n",
      "ID: 1613, Token: atomic\n",
      "ID: 1614, Token: characteristics\n",
      "ID: 1615, Token: columnar\n",
      "ID: 1616, Token: denormalization\n",
      "ID: 1617, Token: differences\n",
      "ID: 1618, Token: etc\n",
      "ID: 1619, Token: main\n",
      "ID: 1620, Token: neo4j\n",
      "ID: 1621, Token: optimize\n",
      "ID: 1622, Token: part\n",
      "ID: 1623, Token: sapiq\n",
      "ID: 1624, Token: value\n",
      "ID: 1625, Token: 000\n",
      "ID: 1626, Token: 110\n",
      "ID: 1627, Token: although\n",
      "ID: 1628, Token: america\n",
      "ID: 1629, Token: ask\n",
      "ID: 1630, Token: assignments\n",
      "ID: 1631, Token: assumes\n",
      "ID: 1632, Token: begin\n",
      "ID: 1633, Token: combine\n",
      "ID: 1634, Token: come\n",
      "ID: 1635, Token: communicating\n",
      "ID: 1636, Token: critically\n",
      "ID: 1637, Token: deliver\n",
      "ID: 1638, Token: down\n",
      "ID: 1639, Token: editor\n",
      "ID: 1640, Token: exponentially\n",
      "ID: 1641, Token: final\n",
      "ID: 1642, Token: glassdoor\n",
      "ID: 1643, Token: governance\n",
      "ID: 1644, Token: gradually\n",
      "ID: 1645, Token: inc\n",
      "ID: 1646, Token: increased\n",
      "ID: 1647, Token: institute\n",
      "ID: 1648, Token: interacting\n",
      "ID: 1649, Token: interpret\n",
      "ID: 1650, Token: landing\n",
      "ID: 1651, Token: mathematician\n",
      "ID: 1652, Token: median\n",
      "ID: 1653, Token: necessary\n",
      "ID: 1654, Token: numbers\n",
      "ID: 1655, Token: occupation\n",
      "ID: 1656, Token: openings\n",
      "ID: 1657, Token: organization\n",
      "ID: 1658, Token: pare\n",
      "ID: 1659, Token: prerequisites\n",
      "ID: 1660, Token: primer\n",
      "ID: 1661, Token: professional\n",
      "ID: 1662, Token: profiling\n",
      "ID: 1663, Token: purposes\n",
      "ID: 1664, Token: recommended\n",
      "ID: 1665, Token: retrieve\n",
      "ID: 1666, Token: salary\n",
      "ID: 1667, Token: sas\n",
      "ID: 1668, Token: scientist\n",
      "ID: 1669, Token: select\n",
      "ID: 1670, Token: shape\n",
      "ID: 1671, Token: skilled\n",
      "ID: 1672, Token: soon\n",
      "ID: 1673, Token: spotter\n",
      "ID: 1674, Token: starts\n",
      "ID: 1675, Token: statements\n",
      "ID: 1676, Token: step\n",
      "ID: 1677, Token: strings\n",
      "ID: 1678, Token: targeted\n",
      "ID: 1679, Token: think\n",
      "ID: 1680, Token: thousands\n",
      "ID: 1681, Token: trend\n",
      "ID: 1682, Token: us\n",
      "ID: 1683, Token: versed\n",
      "ID: 1684, Token: waiting\n",
      "ID: 1685, Token: 3\n",
      "ID: 1686, Token: adaptive\n",
      "ID: 1687, Token: adept\n",
      "ID: 1688, Token: caching\n",
      "ID: 1689, Token: combining\n",
      "ID: 1690, Token: compute\n",
      "ID: 1691, Token: connecting\n",
      "ID: 1692, Token: databricks\n",
      "ID: 1693, Token: delta\n",
      "ID: 1694, Token: distributes\n",
      "ID: 1695, Token: execution\n",
      "ID: 1696, Token: focuses\n",
      "ID: 1697, Token: four\n",
      "ID: 1698, Token: grade\n",
      "ID: 1699, Token: hone\n",
      "ID: 1700, Token: lake\n",
      "ID: 1701, Token: lakehouses\n",
      "ID: 1702, Token: lakes\n",
      "ID: 1703, Token: module\n",
      "ID: 1704, Token: partitions\n",
      "ID: 1705, Token: stage\n",
      "ID: 1706, Token: third\n",
      "ID: 1707, Token: thorough\n",
      "ID: 1708, Token: transitioning\n",
      "ID: 1709, Token: troubleshooting\n",
      "ID: 1710, Token: x\n",
      "ID: 1711, Token: acquired\n",
      "ID: 1712, Token: capstone\n",
      "ID: 1713, Token: choose\n",
      "ID: 1714, Token: consider\n",
      "ID: 1715, Token: demands\n",
      "ID: 1716, Token: feedback\n",
      "ID: 1717, Token: fellow\n",
      "ID: 1718, Token: finish\n",
      "ID: 1719, Token: important\n",
      "ID: 1720, Token: initial\n",
      "ID: 1721, Token: learners\n",
      "ID: 1722, Token: participate\n",
      "ID: 1723, Token: peer\n",
      "ID: 1724, Token: piece\n",
      "ID: 1725, Token: present\n",
      "ID: 1726, Token: proposal\n",
      "ID: 1727, Token: qualitative\n",
      "ID: 1728, Token: receive\n",
      "ID: 1729, Token: review\n",
      "ID: 1730, Token: sense\n",
      "ID: 1731, Token: story\n",
      "ID: 1732, Token: successful\n",
      "ID: 1733, Token: successfully\n",
      "ID: 1734, Token: surface\n",
      "ID: 1735, Token: taking\n",
      "ID: 1736, Token: tell\n",
      "ID: 1737, Token: worthy\n",
      "ID: 1738, Token: designs\n",
      "ID: 1739, Token: diagramming\n",
      "ID: 1740, Token: diagrams\n",
      "ID: 1741, Token: either\n",
      "ID: 1742, Token: ensure\n",
      "ID: 1743, Token: entire\n",
      "ID: 1744, Token: er\n",
      "ID: 1745, Token: erds\n",
      "ID: 1746, Token: excessive\n",
      "ID: 1747, Token: jump\n",
      "ID: 1748, Token: oracle\n",
      "ID: 1749, Token: reporting\n",
      "ID: 1750, Token: requests\n",
      "ID: 1751, Token: satisfy\n",
      "ID: 1752, Token: 13\n",
      "ID: 1753, Token: 14\n",
      "ID: 1754, Token: 15\n",
      "ID: 1755, Token: book\n",
      "ID: 1756, Token: chapters\n",
      "ID: 1757, Token: crawlers\n",
      "ID: 1758, Token: d3\n",
      "ID: 1759, Token: effort\n",
      "ID: 1760, Token: everybody\n",
      "ID: 1761, Token: gathering\n",
      "ID: 1762, Token: material\n",
      "ID: 1763, Token: should\n",
      "ID: 1764, Token: sqlite3\n",
      "ID: 1765, Token: storing\n",
      "ID: 1766, Token: succeed\n",
      "ID: 1767, Token: textbook\n",
      "ID: 1768, Token: accomplish\n",
      "ID: 1769, Token: analysts\n",
      "ID: 1770, Token: clean\n",
      "ID: 1771, Token: cleaning\n",
      "ID: 1772, Token: current\n",
      "ID: 1773, Token: dirty\n",
      "ID: 1774, Token: equipped\n",
      "ID: 1775, Token: following\n",
      "ID: 1776, Token: instruct\n",
      "ID: 1777, Token: introductory\n",
      "ID: 1778, Token: spreadsheets\n",
      "ID: 1779, Token: transforming\n",
      "ID: 1780, Token: verify\n",
      "ID: 1781, Token: acid\n",
      "ID: 1782, Token: advantages\n",
      "ID: 1783, Token: atomicity\n",
      "ID: 1784, Token: configurations\n",
      "ID: 1785, Token: consistency\n",
      "ID: 1786, Token: contrast\n",
      "ID: 1787, Token: deployment\n",
      "ID: 1788, Token: durability\n",
      "ID: 1789, Token: elasticsearch\n",
      "ID: 1790, Token: eventual\n",
      "ID: 1791, Token: examine\n",
      "ID: 1792, Token: implemented\n",
      "ID: 1793, Token: implements\n",
      "ID: 1794, Token: indexes\n",
      "ID: 1795, Token: indexing\n",
      "ID: 1796, Token: isolation\n",
      "ID: 1797, Token: soft\n",
      "ID: 1798, Token: supplement\n",
      "ID: 1799, Token: transactions\n",
      "ID: 1800, Token: articulate\n",
      "ID: 1801, Token: conceptual\n",
      "ID: 1802, Token: conduct\n",
      "ID: 1803, Token: dcl\n",
      "ID: 1804, Token: ddl\n",
      "ID: 1805, Token: define\n",
      "ID: 1806, Token: differentiate\n",
      "ID: 1807, Token: dml\n",
      "ID: 1808, Token: erd\n",
      "ID: 1809, Token: etl\n",
      "ID: 1810, Token: logical\n",
      "ID: 1811, Token: manipulation\n",
      "ID: 1812, Token: nature\n",
      "ID: 1813, Token: physical\n",
      "ID: 1814, Token: progression\n",
      "ID: 1815, Token: tcl\n",
      "ID: 1816, Token: aims\n",
      "ID: 1817, Token: inheritance\n",
      "ID: 1818, Token: lectures\n",
      "ID: 1819, Token: lesson\n",
      "ID: 1820, Token: libaries\n",
      "ID: 1821, Token: prepare\n",
      "ID: 1822, Token: programmer\n",
      "ID: 1823, Token: series\n",
      "ID: 1824, Token: answering\n",
      "ID: 1825, Token: delve\n",
      "ID: 1826, Token: difficulty\n",
      "ID: 1827, Token: impacts\n",
      "ID: 1828, Token: implementations\n",
      "ID: 1829, Token: summary\n",
      "ID: 1830, Token: equivalent\n",
      "ID: 1831, Token: foreign\n",
      "ID: 1832, Token: installation\n",
      "ID: 1833, Token: installing\n",
      "ID: 1834, Token: join\n",
      "ID: 1835, Token: mamp\n",
      "ID: 1836, Token: needed\n",
      "ID: 1837, Token: operation\n",
      "ID: 1838, Token: represent\n",
      "ID: 1839, Token: roles\n",
      "ID: 1840, Token: walk\n",
      "ID: 1841, Token: xampp\n",
      "ID: 1842, Token: blocks\n",
      "ID: 1843, Token: computers\n",
      "ID: 1844, Token: crash\n",
      "ID: 1845, Token: diving\n",
      "ID: 1846, Token: figure\n",
      "ID: 1847, Token: foundations\n",
      "ID: 1848, Token: multitude\n",
      "ID: 1849, Token: off\n",
      "ID: 1850, Token: quickly\n",
      "ID: 1851, Token: 5\n",
      "ID: 1852, Token: avoids\n",
      "ID: 1853, Token: completes\n",
      "ID: 1854, Token: mathematics\n",
      "ID: 1855, Token: moderate\n",
      "ID: 1856, Token: requisites\n",
      "ID: 1857, Token: simplest\n",
      "ID: 1858, Token: 9\n",
      "ID: 1859, Token: accompanying\n",
      "ID: 1860, Token: conditional\n",
      "ID: 1861, Token: debugging\n",
      "ID: 1862, Token: describing\n",
      "ID: 1863, Token: draw\n",
      "ID: 1864, Token: executions\n",
      "ID: 1865, Token: exposition\n",
      "ID: 1866, Token: had\n",
      "ID: 1867, Token: iteration\n",
      "ID: 1868, Token: newcomer\n",
      "ID: 1869, Token: optional\n",
      "ID: 1870, Token: pretty\n",
      "ID: 1871, Token: reason\n",
      "ID: 1872, Token: reasoning\n",
      "ID: 1873, Token: reference\n",
      "ID: 1874, Token: screen\n",
      "ID: 1875, Token: turtle\n",
      "ID: 1876, Token: vocabulary\n",
      "ID: 1877, Token: background\n",
      "ID: 1878, Token: concept\n",
      "ID: 1879, Token: concise\n",
      "ID: 1880, Token: followed\n",
      "ID: 1881, Token: grading\n",
      "ID: 1882, Token: install\n",
      "ID: 1883, Token: instruction\n",
      "ID: 1884, Token: integrated\n",
      "ID: 1885, Token: intended\n",
      "ID: 1886, Token: little\n",
      "ID: 1887, Token: long\n",
      "ID: 1888, Token: pace\n",
      "ID: 1889, Token: preferred\n",
      "ID: 1890, Token: similar\n",
      "ID: 1891, Token: solidify\n",
      "ID: 1892, Token: spyder\n",
      "ID: 1893, Token: submit\n",
      "ID: 1894, Token: tries\n",
      "ID: 1895, Token: coding\n",
      "ID: 1896, Token: conditionals\n",
      "ID: 1897, Token: delves\n",
      "ID: 1898, Token: gets\n",
      "ID: 1899, Token: loops\n",
      "ID: 1900, Token: reading\n",
      "ID: 1901, Token: robust\n",
      "ID: 1902, Token: variables\n",
      "ID: 1903, Token: achieve\n",
      "ID: 1904, Token: applies\n",
      "ID: 1905, Token: columns\n",
      "ID: 1906, Token: confidence\n",
      "ID: 1907, Token: configured\n",
      "ID: 1908, Token: distribution\n",
      "ID: 1909, Token: due\n",
      "ID: 1910, Token: evaluate\n",
      "ID: 1911, Token: frequency\n",
      "ID: 1912, Token: generating\n",
      "ID: 1913, Token: indicators\n",
      "ID: 1914, Token: industry\n",
      "ID: 1915, Token: interval\n",
      "ID: 1916, Token: manipulate\n",
      "ID: 1917, Token: population\n",
      "ID: 1918, Token: practicing\n",
      "ID: 1919, Token: readability\n",
      "ID: 1920, Token: recall\n",
      "ID: 1921, Token: save\n",
      "ID: 1922, Token: simplicity\n",
      "ID: 1923, Token: stock\n",
      "ID: 1924, Token: variable\n",
      "ID: 1925, Token: applied\n",
      "ID: 1926, Token: approaches\n",
      "ID: 1927, Token: carry\n",
      "ID: 1928, Token: charting\n",
      "ID: 1929, Token: clusters\n",
      "ID: 1930, Token: described\n",
      "ID: 1931, Token: discussed\n",
      "ID: 1932, Token: discussion\n",
      "ID: 1933, Token: e\n",
      "ID: 1934, Token: engineer\n",
      "ID: 1935, Token: ensembles\n",
      "ID: 1936, Token: evaluating\n",
      "ID: 1937, Token: focusing\n",
      "ID: 1938, Token: g\n",
      "ID: 1939, Token: generalizability\n",
      "ID: 1940, Token: issue\n",
      "ID: 1941, Token: overfitting\n",
      "ID: 1942, Token: particular\n",
      "ID: 1943, Token: plotting\n",
      "ID: 1944, Token: practical\n",
      "ID: 1945, Token: scikit\n",
      "ID: 1946, Token: tackled\n",
      "ID: 1947, Token: taken\n",
      "ID: 1948, Token: toolkit\n",
      "ID: 1949, Token: abstraction\n",
      "ID: 1950, Token: analyses\n",
      "ID: 1951, Token: central\n",
      "ID: 1952, Token: csv\n",
      "ID: 1953, Token: effectively\n",
      "ID: 1954, Token: groupby\n",
      "ID: 1955, Token: inferential\n",
      "ID: 1956, Token: lambdas\n",
      "ID: 1957, Token: manipulating\n",
      "ID: 1958, Token: merge\n",
      "ID: 1959, Token: numpy\n",
      "ID: 1960, Token: pivot\n",
      "ID: 1961, Token: tabular\n",
      "ID: 1962, Token: aggregation\n",
      "ID: 1963, Token: answer\n",
      "ID: 1964, Token: inspecting\n",
      "ID: 1965, Token: joining\n",
      "ID: 1966, Token: matplotlib\n",
      "ID: 1967, Token: summarization\n",
      "ID: 1968, Token: across\n",
      "ID: 1969, Token: audiences\n",
      "ID: 1970, Token: aws\n",
      "ID: 1971, Token: block\n",
      "ID: 1972, Token: cdn\n",
      "ID: 1973, Token: com\n",
      "ID: 1974, Token: completion\n",
      "ID: 1975, Token: definition\n",
      "ID: 1976, Token: eligible\n",
      "ID: 1977, Token: emergent\n",
      "ID: 1978, Token: enabled\n",
      "ID: 1979, Token: executive\n",
      "ID: 1980, Token: go\n",
      "ID: 1981, Token: grounding\n",
      "ID: 1982, Token: https\n",
      "ID: 1983, Token: iaas\n",
      "ID: 1984, Token: infrastructure\n",
      "ID: 1985, Token: makes\n",
      "ID: 1986, Token: microsoft\n",
      "ID: 1987, Token: networking\n",
      "ID: 1988, Token: offer\n",
      "ID: 1989, Token: org\n",
      "ID: 1990, Token: paas\n",
      "ID: 1991, Token: perspective\n",
      "ID: 1992, Token: prior\n",
      "ID: 1993, Token: private\n",
      "ID: 1994, Token: prominent\n",
      "ID: 1995, Token: providers\n",
      "ID: 1996, Token: provisioning\n",
      "ID: 1997, Token: require\n",
      "ID: 1998, Token: saas\n",
      "ID: 1999, Token: someone\n",
      "ID: 2000, Token: studies\n",
      "ID: 2001, Token: suitable\n",
      "ID: 2002, Token: though\n",
      "ID: 2003, Token: usecases\n",
      "ID: 2004, Token: verticals\n",
      "ID: 2005, Token: vms\n",
      "ID: 2006, Token: www\n",
      "ID: 2007, Token: youracclaim\n",
      "ID: 2008, Token: azure\n",
      "ID: 2009, Token: delivered\n",
      "ID: 2010, Token: economics\n",
      "ID: 2011, Token: hosting\n",
      "ID: 2012, Token: last\n",
      "ID: 2013, Token: least\n",
      "ID: 2014, Token: levels\n",
      "ID: 2015, Token: beginners\n",
      "ID: 2016, Token: github\n",
      "ID: 2017, Token: global\n",
      "ID: 2018, Token: hosted\n",
      "ID: 2019, Token: hugo\n",
      "ID: 2020, Token: infrastructures\n",
      "ID: 2021, Token: intermediate\n",
      "ID: 2022, Token: involving\n",
      "ID: 2023, Token: solutions\n",
      "ID: 2024, Token: statically\n",
      "ID: 2025, Token: website\n",
      "ID: 2026, Token: websites\n",
      "ID: 2027, Token: academia\n",
      "ID: 2028, Token: centered\n",
      "ID: 2029, Token: classical\n",
      "ID: 2030, Token: clouds\n",
      "ID: 2031, Token: homework\n",
      "ID: 2032, Token: inside\n",
      "ID: 2033, Token: interviews\n",
      "ID: 2034, Token: managers\n",
      "ID: 2035, Token: much\n",
      "ID: 2036, Token: philosophies\n",
      "ID: 2037, Token: researchers\n",
      "ID: 2038, Token: stores\n",
      "ID: 2039, Token: trending\n",
      "ID: 2040, Token: widely\n",
      "ID: 2041, Token: offers\n",
      "ID: 2042, Token: primary\n",
      "ID: 2043, Token: second\n",
      "ID: 2044, Token: 4\n",
      "ID: 2045, Token: amazon\n",
      "ID: 2046, Token: archival\n",
      "ID: 2047, Token: balancing\n",
      "ID: 2048, Token: ceph\n",
      "ID: 2049, Token: change\n",
      "ID: 2050, Token: comparing\n",
      "ID: 2051, Token: comprise\n",
      "ID: 2052, Token: defined\n",
      "ID: 2053, Token: drives\n",
      "ID: 2054, Token: dropbox\n",
      "ID: 2055, Token: higher\n",
      "ID: 2056, Token: introducing\n",
      "ID: 2057, Token: jvm\n",
      "ID: 2058, Token: kubernates\n",
      "ID: 2059, Token: maas\n",
      "ID: 2060, Token: major\n",
      "ID: 2061, Token: metal\n",
      "ID: 2062, Token: middleware\n",
      "ID: 2063, Token: modern\n",
      "ID: 2064, Token: moves\n",
      "ID: 2065, Token: organize\n",
      "ID: 2066, Token: provisioned\n",
      "ID: 2067, Token: pure\n",
      "ID: 2068, Token: revolution\n",
      "ID: 2069, Token: rpc\n",
      "ID: 2070, Token: special\n",
      "ID: 2071, Token: talk\n",
      "ID: 2072, Token: view\n",
      "ID: 2073, Token: virtualization\n",
      "ID: 2074, Token: week\n",
      "ID: 2075, Token: wraps\n",
      "ID: 2076, Token: years\n",
      "ID: 2077, Token: bayes\n",
      "ID: 2078, Token: centers\n",
      "ID: 2079, Token: cloudera\n",
      "ID: 2080, Token: cntk\n",
      "ID: 2081, Token: construction\n",
      "ID: 2082, Token: difficulties\n",
      "ID: 2083, Token: disks\n",
      "ID: 2084, Token: disruptive\n",
      "ID: 2085, Token: distributions\n",
      "ID: 2086, Token: enormous\n",
      "ID: 2087, Token: finding\n",
      "ID: 2088, Token: flexible\n",
      "ID: 2089, Token: flow\n",
      "ID: 2090, Token: fpm\n",
      "ID: 2091, Token: giraph\n",
      "ID: 2092, Token: given\n",
      "ID: 2093, Token: hortonworks\n",
      "ID: 2094, Token: huge\n",
      "ID: 2095, Token: ideas\n",
      "ID: 2096, Token: kappa\n",
      "ID: 2097, Token: kmeans\n",
      "ID: 2098, Token: lambda\n",
      "ID: 2099, Token: log\n",
      "ID: 2100, Token: mahout\n",
      "ID: 2101, Token: mapr\n",
      "ID: 2102, Token: memories\n",
      "ID: 2103, Token: middle\n",
      "ID: 2104, Token: mxnet\n",
      "ID: 2105, Token: naive\n",
      "ID: 2106, Token: opens\n",
      "ID: 2107, Token: paxos\n",
      "ID: 2108, Token: pregel\n",
      "ID: 2109, Token: processors\n",
      "ID: 2110, Token: programmability\n",
      "ID: 2111, Token: quantities\n",
      "ID: 2112, Token: redis\n",
      "ID: 2113, Token: society\n",
      "ID: 2114, Token: storm\n",
      "ID: 2115, Token: streamed\n",
      "ID: 2116, Token: subscribe\n",
      "ID: 2117, Token: theme\n",
      "ID: 2118, Token: velocity\n",
      "ID: 2119, Token: visit\n",
      "ID: 2120, Token: volumes\n",
      "ID: 2121, Token: comes\n",
      "ID: 2122, Token: daily\n",
      "ID: 2123, Token: entails\n",
      "ID: 2124, Token: exactly\n",
      "ID: 2125, Token: exciting\n",
      "ID: 2126, Token: life\n",
      "ID: 2127, Token: maker\n",
      "ID: 2128, Token: mastering\n",
      "ID: 2129, Token: option\n",
      "ID: 2130, Token: paths\n",
      "ID: 2131, Token: profession\n",
      "ID: 2132, Token: responsibilities\n",
      "ID: 2133, Token: scenario\n",
      "ID: 2134, Token: throughout\n",
      "ID: 2135, Token: vendors\n",
      "ID: 2136, Token: aspiring\n",
      "ID: 2137, Token: browser\n",
      "ID: 2138, Token: cleansing\n",
      "ID: 2139, Token: cost\n",
      "ID: 2140, Token: demonstrated\n",
      "ID: 2141, Token: desktop\n",
      "ID: 2142, Token: domains\n",
      "ID: 2143, Token: downloads\n",
      "ID: 2144, Token: excel\n",
      "ID: 2145, Token: expand\n",
      "ID: 2146, Token: follow\n",
      "ID: 2147, Token: formatting\n",
      "ID: 2148, Token: however\n",
      "ID: 2149, Token: live\n",
      "ID: 2150, Token: made\n",
      "ID: 2151, Token: marketing\n",
      "ID: 2152, Token: newly\n",
      "ID: 2153, Token: nor\n",
      "ID: 2154, Token: organized\n",
      "ID: 2155, Token: plenty\n",
      "ID: 2156, Token: quite\n",
      "ID: 2157, Token: readable\n",
      "ID: 2158, Token: research\n",
      "ID: 2159, Token: sheets\n",
      "ID: 2160, Token: sorting\n",
      "ID: 2161, Token: strong\n",
      "ID: 2162, Token: usage\n",
      "ID: 2163, Token: worked\n",
      "ID: 2164, Token: wrangling\n",
      "ID: 2165, Token: achieved\n",
      "ID: 2166, Token: acquainted\n",
      "ID: 2167, Token: activity\n",
      "ID: 2168, Token: discipline\n",
      "ID: 2169, Token: his\n",
      "ID: 2170, Token: human\n",
      "ID: 2171, Token: researcher\n",
      "ID: 2172, Token: solved\n",
      "ID: 2173, Token: solving\n",
      "ID: 2174, Token: spheres\n",
      "ID: 2175, Token: underlying\n",
      "ID: 2176, Token: combination\n",
      "ID: 2177, Token: dealing\n",
      "ID: 2178, Token: during\n",
      "ID: 2179, Token: exponential\n",
      "ID: 2180, Token: fitted\n",
      "ID: 2181, Token: forecasts\n",
      "ID: 2182, Token: goodness\n",
      "ID: 2183, Token: holt\n",
      "ID: 2184, Token: intuition\n",
      "ID: 2185, Token: minnesota\n",
      "ID: 2186, Token: missing\n",
      "ID: 2187, Token: opportunities\n",
      "ID: 2188, Token: selection\n",
      "ID: 2189, Token: smoothing\n",
      "ID: 2190, Token: underfitting\n",
      "ID: 2191, Token: values\n",
      "ID: 2192, Token: winter\n",
      "ID: 2193, Token: equip\n",
      "ID: 2194, Token: lets\n",
      "ID: 2195, Token: markdown\n",
      "ID: 2196, Token: packages\n",
      "ID: 2197, Token: seventh\n",
      "ID: 2198, Token: tidyverse\n",
      "ID: 2199, Token: 6\n",
      "ID: 2200, Token: importing\n",
      "ID: 2201, Token: lecture\n",
      "ID: 2202, Token: meaningful\n",
      "ID: 2203, Token: parts\n",
      "ID: 2204, Token: scipy\n",
      "ID: 2205, Token: summarizing\n",
      "ID: 2206, Token: adding\n",
      "ID: 2207, Token: analyse\n",
      "ID: 2208, Token: attracted\n",
      "ID: 2209, Token: automate\n",
      "ID: 2210, Token: becomes\n",
      "ID: 2211, Token: broad\n",
      "ID: 2212, Token: categorise\n",
      "ID: 2213, Token: content\n",
      "ID: 2214, Token: coursera\n",
      "ID: 2215, Token: currency\n",
      "ID: 2216, Token: efficient\n",
      "ID: 2217, Token: extraordinarily\n",
      "ID: 2218, Token: hundreds\n",
      "ID: 2219, Token: importantly\n",
      "ID: 2220, Token: later\n",
      "ID: 2221, Token: link\n",
      "ID: 2222, Token: lookup\n",
      "ID: 2223, Token: named\n",
      "ID: 2224, Token: paramount\n",
      "ID: 2225, Token: personalize\n",
      "ID: 2226, Token: ranges\n",
      "ID: 2227, Token: ratings\n",
      "ID: 2228, Token: repertoire\n",
      "ID: 2229, Token: sequel\n",
      "ID: 2230, Token: she\n",
      "ID: 2231, Token: specializations\n",
      "ID: 2232, Token: trials\n",
      "ID: 2233, Token: tribulations\n",
      "ID: 2234, Token: zara\n",
      "ID: 2235, Token: acquiring\n",
      "ID: 2236, Token: calculus\n",
      "ID: 2237, Token: categorical\n",
      "ID: 2238, Token: detecting\n",
      "ID: 2239, Token: familiarity\n",
      "ID: 2240, Token: hypothesis\n",
      "ID: 2241, Token: ordinal\n",
      "ID: 2242, Token: outliers\n",
      "ID: 2243, Token: quality\n",
      "ID: 2244, Token: realize\n",
      "ID: 2245, Token: targets\n",
      "ID: 2246, Token: aim\n",
      "ID: 2247, Token: functioning\n",
      "ID: 2248, Token: keeping\n",
      "ID: 2249, Token: mind\n",
      "ID: 2250, Token: organizing\n",
      "ID: 2251, Token: peripheral\n",
      "ID: 2252, Token: regularly\n",
      "ID: 2253, Token: widespread\n",
      "ID: 2254, Token: ad\n",
      "ID: 2255, Token: constructing\n",
      "ID: 2256, Token: datafframes\n",
      "ID: 2257, Token: deal\n",
      "ID: 2258, Token: disk\n",
      "ID: 2259, Token: doubt\n",
      "ID: 2260, Token: executing\n",
      "ID: 2261, Token: graphframes\n",
      "ID: 2262, Token: graphs\n",
      "ID: 2263, Token: hoc\n",
      "ID: 2264, Token: leaving\n",
      "ID: 2265, Token: maximum\n",
      "ID: 2266, Token: mountain\n",
      "ID: 2267, Token: optimizing\n",
      "ID: 2268, Token: partially\n",
      "ID: 2269, Token: precisely\n",
      "ID: 2270, Token: productive\n",
      "ID: 2271, Token: space\n",
      "ID: 2272, Token: stones\n",
      "ID: 2273, Token: stop\n",
      "ID: 2274, Token: strain\n",
      "ID: 2275, Token: struggling\n",
      "ID: 2276, Token: tackle\n",
      "ID: 2277, Token: thus\n",
      "ID: 2278, Token: transformations\n",
      "ID: 2279, Token: translated\n",
      "ID: 2280, Token: warehouse\n",
      "ID: 2281, Token: workflow\n",
      "ID: 2282, Token: yourself\n",
      "ID: 2283, Token: alerts\n",
      "ID: 2284, Token: containerized\n",
      "ID: 2285, Token: continuously\n",
      "ID: 2286, Token: deployed\n",
      "ID: 2287, Token: flask\n",
      "ID: 2288, Token: gcp\n",
      "ID: 2289, Token: kaizen\n",
      "ID: 2290, Token: aca\n",
      "ID: 2291, Token: alibaba\n",
      "ID: 2292, Token: biggest\n",
      "ID: 2293, Token: certification\n",
      "ID: 2294, Token: commercial\n",
      "ID: 2295, Token: consist\n",
      "ID: 2296, Token: contain\n",
      "ID: 2297, Token: demonstrates\n",
      "ID: 2298, Token: ecr\n",
      "ID: 2299, Token: ecs\n",
      "ID: 2300, Token: eks\n",
      "ID: 2301, Token: elastic\n",
      "ID: 2302, Token: exam\n",
      "ID: 2303, Token: executable\n",
      "ID: 2304, Token: existed\n",
      "ID: 2305, Token: expert\n",
      "ID: 2306, Token: extra\n",
      "ID: 2307, Token: function\n",
      "ID: 2308, Token: helps\n",
      "ID: 2309, Token: implementation\n",
      "ID: 2310, Token: instructors\n",
      "ID: 2311, Token: lightsail\n",
      "ID: 2312, Token: macro\n",
      "ID: 2313, Token: modernize\n",
      "ID: 2314, Token: monolithic\n",
      "ID: 2315, Token: orchestrate\n",
      "ID: 2316, Token: popularity\n",
      "ID: 2317, Token: prevalent\n",
      "ID: 2318, Token: regardless\n",
      "ID: 2319, Token: registry\n",
      "ID: 2320, Token: starting\n",
      "ID: 2321, Token: still\n",
      "ID: 2322, Token: stopping\n",
      "ID: 2323, Token: traditional\n",
      "ID: 2324, Token: trainers\n",
      "ID: 2325, Token: unlike\n",
      "ID: 2326, Token: video\n",
      "ID: 2327, Token: currently\n",
      "ID: 2328, Token: daemon\n",
      "ID: 2329, Token: demand\n",
      "ID: 2330, Token: devoted\n",
      "ID: 2331, Token: drill\n",
      "ID: 2332, Token: fifth\n",
      "ID: 2333, Token: highest\n",
      "ID: 2334, Token: ports\n",
      "ID: 2335, Token: whatever\n",
      "ID: 2336, Token: act\n",
      "ID: 2337, Token: anti\n",
      "ID: 2338, Token: audio\n",
      "ID: 2339, Token: bias\n",
      "ID: 2340, Token: cars\n",
      "ID: 2341, Token: datamining\n",
      "ID: 2342, Token: decade\n",
      "ID: 2343, Token: dozens\n",
      "ID: 2344, Token: driving\n",
      "ID: 2345, Token: explicitly\n",
      "ID: 2346, Token: genome\n",
      "ID: 2347, Token: iii\n",
      "ID: 2348, Token: improved\n",
      "ID: 2349, Token: informatics\n",
      "ID: 2350, Token: innovation\n",
      "ID: 2351, Token: kernels\n",
      "ID: 2352, Token: knowing\n",
      "ID: 2353, Token: numerous\n",
      "ID: 2354, Token: parametric\n",
      "ID: 2355, Token: past\n",
      "ID: 2356, Token: pattern\n",
      "ID: 2357, Token: perception\n",
      "ID: 2358, Token: pertains\n",
      "ID: 2359, Token: pervasive\n",
      "ID: 2360, Token: powerfully\n",
      "ID: 2361, Token: probably\n",
      "ID: 2362, Token: programmed\n",
      "ID: 2363, Token: progress\n",
      "ID: 2364, Token: self\n",
      "ID: 2365, Token: silicon\n",
      "ID: 2366, Token: theoretical\n",
      "ID: 2367, Token: theory\n",
      "ID: 2368, Token: towards\n",
      "ID: 2369, Token: underpinnings\n",
      "ID: 2370, Token: valley\n",
      "ID: 2371, Token: vastly\n",
      "ID: 2372, Token: vector\n",
      "ID: 2373, Token: vision\n",
      "ID: 2374, Token: actually\n",
      "ID: 2375, Token: affect\n",
      "ID: 2376, Token: believe\n",
      "ID: 2377, Token: breakthroughs\n",
      "ID: 2378, Token: cocacola\n",
      "ID: 2379, Token: considering\n",
      "ID: 2380, Token: details\n",
      "ID: 2381, Token: diagnostics\n",
      "ID: 2382, Token: ebay\n",
      "ID: 2383, Token: facial\n",
      "ID: 2384, Token: fiction\n",
      "ID: 2385, Token: goldsmiths\n",
      "ID: 2386, Token: guiding\n",
      "ID: 2387, Token: herald\n",
      "ID: 2388, Token: hottest\n",
      "ID: 2389, Token: involve\n",
      "ID: 2390, Token: likely\n",
      "ID: 2391, Token: lives\n",
      "ID: 2392, Token: logistic\n",
      "ID: 2393, Token: london\n",
      "ID: 2394, Token: math\n",
      "ID: 2395, Token: moment\n",
      "ID: 2396, Token: multilayer\n",
      "ID: 2397, Token: nb\n",
      "ID: 2398, Token: needing\n",
      "ID: 2399, Token: often\n",
      "ID: 2400, Token: perceptrons\n",
      "ID: 2401, Token: prediction\n",
      "ID: 2402, Token: really\n",
      "ID: 2403, Token: recognise\n",
      "ID: 2404, Token: reputation\n",
      "ID: 2405, Token: requiring\n",
      "ID: 2406, Token: revolutionise\n",
      "ID: 2407, Token: snapchat\n",
      "ID: 2408, Token: stories\n",
      "ID: 2409, Token: tech\n",
      "ID: 2410, Token: uber\n",
      "ID: 2411, Token: whoever\n",
      "ID: 2412, Token: entities\n",
      "ID: 2413, Token: fastest\n",
      "ID: 2414, Token: fields\n",
      "ID: 2415, Token: furthering\n",
      "ID: 2416, Token: instrumental\n",
      "ID: 2417, Token: secret\n",
      "ID: 2418, Token: syntactic\n",
      "ID: 2419, Token: transcription\n",
      "ID: 2420, Token: accuracy\n",
      "ID: 2421, Token: agility\n",
      "ID: 2422, Token: automated\n",
      "ID: 2423, Token: bert\n",
      "ID: 2424, Token: elasticity\n",
      "ID: 2425, Token: exceeds\n",
      "ID: 2426, Token: fine\n",
      "ID: 2427, Token: focused\n",
      "ID: 2428, Token: geared\n",
      "ID: 2429, Token: highly\n",
      "ID: 2430, Token: hugging\n",
      "ID: 2431, Token: local\n",
      "ID: 2432, Token: loop\n",
      "ID: 2433, Token: massive\n",
      "ID: 2434, Token: minimum\n",
      "ID: 2435, Token: originate\n",
      "ID: 2436, Token: pipeline\n",
      "ID: 2437, Token: sagemaker\n",
      "ID: 2438, Token: store\n",
      "ID: 2439, Token: threshold\n",
      "ID: 2440, Token: tune\n",
      "ID: 2441, Token: wikipedia\n",
      "ID: 2442, Token: baseline\n",
      "ID: 2443, Token: drift\n",
      "ID: 2444, Token: expertise\n",
      "ID: 2445, Token: improving\n",
      "ID: 2446, Token: productionized\n",
      "ID: 2447, Token: prototype\n",
      "ID: 2448, Token: scoping\n",
      "ID: 2449, Token: strategies\n",
      "ID: 2450, Token: assessing\n",
      "ID: 2451, Token: evolution\n",
      "ID: 2452, Token: extended\n",
      "ID: 2453, Token: lineage\n",
      "ID: 2454, Token: metadata\n",
      "ID: 2455, Token: provenance\n",
      "ID: 2456, Token: automation\n",
      "ID: 2457, Token: avoid\n",
      "ID: 2458, Token: batch\n",
      "ID: 2459, Token: complies\n",
      "ID: 2460, Token: decay\n",
      "ID: 2461, Token: delivery\n",
      "ID: 2462, Token: depending\n",
      "ID: 2463, Token: drops\n",
      "ID: 2464, Token: fourth\n",
      "ID: 2465, Token: keep\n",
      "ID: 2466, Token: logging\n",
      "ID: 2467, Token: mlops\n",
      "ID: 2468, Token: operate\n",
      "ID: 2469, Token: progressive\n",
      "ID: 2470, Token: remediate\n",
      "ID: 2471, Token: serving\n",
      "ID: 2472, Token: 50\n",
      "ID: 2473, Token: activation\n",
      "ID: 2474, Token: alexnet\n",
      "ID: 2475, Token: cnn\n",
      "ID: 2476, Token: d\n",
      "ID: 2477, Token: duck\n",
      "ID: 2478, Token: famous\n",
      "ID: 2479, Token: fcn\n",
      "ID: 2480, Token: label\n",
      "ID: 2481, Token: localization\n",
      "ID: 2482, Token: localize\n",
      "ID: 2483, Token: mask\n",
      "ID: 2484, Token: net\n",
      "ID: 2485, Token: pets\n",
      "ID: 2486, Token: rcnn\n",
      "ID: 2487, Token: regional\n",
      "ID: 2488, Token: resnet\n",
      "ID: 2489, Token: rubber\n",
      "ID: 2490, Token: saliency\n",
      "ID: 2491, Token: segmentation\n",
      "ID: 2492, Token: transfer\n",
      "ID: 2493, Token: u\n",
      "ID: 2494, Token: variations\n",
      "ID: 2495, Token: zombies\n",
      "ID: 2496, Token: admissions\n",
      "ID: 2497, Token: boulder\n",
      "ID: 2498, Token: classic\n",
      "ID: 2499, Token: cu\n",
      "ID: 2500, Token: departments\n",
      "ID: 2501, Token: drawbacks\n",
      "ID: 2502, Token: ds\n",
      "ID: 2503, Token: estimation\n",
      "ID: 2504, Token: faculty\n",
      "ID: 2505, Token: individuals\n",
      "ID: 2506, Token: interdisciplinary\n",
      "ID: 2507, Token: ms\n",
      "ID: 2508, Token: others\n",
      "ID: 2509, Token: pose\n",
      "ID: 2510, Token: suggested\n",
      "ID: 2511, Token: undergraduate\n",
      "ID: 2512, Token: added\n",
      "ID: 2513, Token: boost\n",
      "ID: 2514, Token: editing\n",
      "ID: 2515, Token: everyday\n",
      "ID: 2516, Token: internal\n",
      "ID: 2517, Token: mechanics\n",
      "ID: 2518, Token: motion\n",
      "ID: 2519, Token: movies\n",
      "ID: 2520, Token: renown\n",
      "ID: 2521, Token: stylization\n",
      "ID: 2522, Token: turning\n",
      "ID: 2523, Token: tv\n",
      "ID: 2524, Token: 3d\n",
      "ID: 2525, Token: co\n",
      "ID: 2526, Token: color\n",
      "ID: 2527, Token: crucial\n",
      "ID: 2528, Token: curious\n",
      "ID: 2529, Token: derivatives\n",
      "ID: 2530, Token: desire\n",
      "ID: 2531, Token: else\n",
      "ID: 2532, Token: formation\n",
      "ID: 2533, Token: html\n",
      "ID: 2534, Token: humans\n",
      "ID: 2535, Token: imaging\n",
      "ID: 2536, Token: mathworks\n",
      "ID: 2537, Token: matlab\n",
      "ID: 2538, Token: matrix\n",
      "ID: 2539, Token: mid\n",
      "ID: 2540, Token: mission\n",
      "ID: 2541, Token: neuroscience\n",
      "ID: 2542, Token: notation\n",
      "ID: 2543, Token: onramp\n",
      "ID: 2544, Token: ordinate\n",
      "ID: 2545, Token: receiving\n",
      "ID: 2546, Token: signal\n",
      "ID: 2547, Token: astronomical\n",
      "ID: 2548, Token: bio\n",
      "ID: 2549, Token: blur\n",
      "ID: 2550, Token: car\n",
      "ID: 2551, Token: cases\n",
      "ID: 2552, Token: compression\n",
      "ID: 2553, Token: continues\n",
      "ID: 2554, Token: degradations\n",
      "ID: 2555, Token: dimensional\n",
      "ID: 2556, Token: economical\n",
      "ID: 2557, Token: electromagnetic\n",
      "ID: 2558, Token: emphasis\n",
      "ID: 2559, Token: enhancement\n",
      "ID: 2560, Token: experiencing\n",
      "ID: 2561, Token: gamma\n",
      "ID: 2562, Token: industrial\n",
      "ID: 2563, Token: infrared\n",
      "ID: 2564, Token: interests\n",
      "ID: 2565, Token: moreover\n",
      "ID: 2566, Token: multimedia\n",
      "ID: 2567, Token: pertaining\n",
      "ID: 2568, Token: plays\n",
      "ID: 2569, Token: rays\n",
      "ID: 2570, Token: recovery\n",
      "ID: 2571, Token: removal\n",
      "ID: 2572, Token: removing\n",
      "ID: 2573, Token: scientific\n",
      "ID: 2574, Token: signals\n",
      "ID: 2575, Token: skill\n",
      "ID: 2576, Token: sparsity\n",
      "ID: 2577, Token: spatial\n",
      "ID: 2578, Token: spatio\n",
      "ID: 2579, Token: spectrum\n",
      "ID: 2580, Token: therefore\n",
      "ID: 2581, Token: toolboxes\n",
      "ID: 2582, Token: transmission\n",
      "ID: 2583, Token: utilized\n",
      "ID: 2584, Token: visible\n",
      "ID: 2585, Token: andrew\n",
      "ID: 2586, Token: deeplearning\n",
      "ID: 2587, Token: ng\n",
      "ID: 2588, Token: upcoming\n",
      "ID: 2589, Token: augmentation\n",
      "ID: 2590, Token: convolutions\n",
      "ID: 2591, Token: dropout\n",
      "ID: 2592, Token: extracted\n",
      "ID: 2593, Token: loss\n",
      "ID: 2594, Token: plot\n",
      "ID: 2595, Token: prevent\n",
      "ID: 2596, Token: sees\n",
      "ID: 2597, Token: shapes\n",
      "ID: 2598, Token: sizes\n",
      "ID: 2599, Token: aside\n",
      "ID: 2600, Token: convenient\n",
      "ID: 2601, Token: definitely\n",
      "ID: 2602, Token: eventually\n",
      "ID: 2603, Token: fluff\n",
      "ID: 2604, Token: left\n",
      "ID: 2605, Token: mean\n",
      "ID: 2606, Token: possible\n",
      "ID: 2607, Token: sacrificing\n",
      "ID: 2608, Token: team\n",
      "ID: 2609, Token: unsuccessful\n",
      "ID: 2610, Token: actionable\n",
      "ID: 2611, Token: active\n",
      "ID: 2612, Token: analytic\n",
      "ID: 2613, Token: assembling\n",
      "ID: 2614, Token: assumptions\n",
      "ID: 2615, Token: causal\n",
      "ID: 2616, Token: challenge\n",
      "ID: 2617, Token: clear\n",
      "ID: 2618, Token: clearly\n",
      "ID: 2619, Token: conclusions\n",
      "ID: 2620, Token: confounding\n",
      "ID: 2621, Token: contrasting\n",
      "ID: 2622, Token: counterfactuals\n",
      "ID: 2623, Token: errors\n",
      "ID: 2624, Token: facing\n",
      "ID: 2625, Token: glimpse\n",
      "ID: 2626, Token: happened\n",
      "ID: 2627, Token: happens\n",
      "ID: 2628, Token: hypotheses\n",
      "ID: 2629, Token: merging\n",
      "ID: 2630, Token: messy\n",
      "ID: 2631, Token: obvious\n",
      "ID: 2632, Token: outlined\n",
      "ID: 2633, Token: perfectly\n",
      "ID: 2634, Token: performed\n",
      "ID: 2635, Token: pitfalls\n",
      "ID: 2636, Token: plan\n",
      "ID: 2637, Token: pull\n",
      "ID: 2638, Token: pulling\n",
      "ID: 2639, Token: pulls\n",
      "ID: 2640, Token: randomization\n",
      "ID: 2641, Token: statisticians\n",
      "ID: 2642, Token: strengths\n",
      "ID: 2643, Token: treatment\n",
      "ID: 2644, Token: versus\n",
      "ID: 2645, Token: weaknesses\n",
      "ID: 2646, Token: went\n",
      "ID: 2647, Token: were\n",
      "ID: 2648, Token: assess\n",
      "ID: 2649, Token: conducted\n",
      "ID: 2650, Token: drawn\n",
      "ID: 2651, Token: employers\n",
      "ID: 2652, Token: government\n",
      "ID: 2653, Token: partners\n",
      "ID: 2654, Token: potential\n",
      "ID: 2655, Token: usable\n",
      "ID: 2656, Token: collaboration\n",
      "ID: 2657, Token: datacamp\n",
      "ID: 2658, Token: eds\n",
      "ID: 2659, Token: evaluated\n",
      "ID: 2660, Token: graded\n",
      "ID: 2661, Token: lead\n",
      "ID: 2662, Token: marketplace\n",
      "ID: 2663, Token: participants\n",
      "ID: 2664, Token: rental\n",
      "ID: 2665, Token: shepherd\n",
      "ID: 2666, Token: zillow\n",
      "ID: 2667, Token: accessible\n",
      "ID: 2668, Token: architectural\n",
      "ID: 2669, Token: conversant\n",
      "ID: 2670, Token: easier\n",
      "ID: 2671, Token: era\n",
      "ID: 2672, Token: explanation\n",
      "ID: 2673, Token: recast\n",
      "ID: 2674, Token: sensors\n",
      "ID: 2675, Token: thinking\n",
      "ID: 2676, Token: utilize\n",
      "ID: 2677, Token: v\n",
      "ID: 2678, Token: valence\n",
      "ID: 2679, Token: veracity\n",
      "ID: 2680, Token: appreciate\n",
      "ID: 2681, Token: appropriate\n",
      "ID: 2682, Token: asterixdb\n",
      "ID: 2683, Token: collect\n",
      "ID: 2684, Token: connections\n",
      "ID: 2685, Token: evolving\n",
      "ID: 2686, Token: frequent\n",
      "ID: 2687, Token: genres\n",
      "ID: 2688, Token: hp\n",
      "ID: 2689, Token: identified\n",
      "ID: 2690, Token: impala\n",
      "ID: 2691, Token: intro\n",
      "ID: 2692, Token: plethora\n",
      "ID: 2693, Token: refer\n",
      "ID: 2694, Token: semi\n",
      "ID: 2695, Token: sparksql\n",
      "ID: 2696, Token: specifications\n",
      "ID: 2697, Token: suit\n",
      "ID: 2698, Token: untapped\n",
      "ID: 2699, Token: vertica\n",
      "ID: 2700, Token: construct\n",
      "ID: 2701, Token: incorporate\n",
      "ID: 2702, Token: bring\n",
      "ID: 2703, Token: catch\n",
      "ID: 2704, Token: compelling\n",
      "ID: 2705, Token: earlier\n",
      "ID: 2706, Token: engaging\n",
      "ID: 2707, Token: flamingo\n",
      "ID: 2708, Token: generated\n",
      "ID: 2709, Token: gephi\n",
      "ID: 2710, Token: imaginary\n",
      "ID: 2711, Token: knime\n",
      "ID: 2712, Token: leadership\n",
      "ID: 2713, Token: office\n",
      "ID: 2714, Token: pink\n",
      "ID: 2715, Token: preparing\n",
      "ID: 2716, Token: presentations\n",
      "ID: 2717, Token: recruiters\n",
      "ID: 2718, Token: simulating\n",
      "ID: 2719, Token: slide\n",
      "ID: 2720, Token: splunk\n",
      "ID: 2721, Token: computational\n",
      "ID: 2722, Token: exist\n",
      "ID: 2723, Token: finance\n",
      "ID: 2724, Token: grasp\n",
      "ID: 2725, Token: internals\n",
      "ID: 2726, Token: miss\n",
      "ID: 2727, Token: missed\n",
      "ID: 2728, Token: namely\n",
      "ID: 2729, Token: serve\n",
      "ID: 2730, Token: telecommunications\n",
      "ID: 2731, Token: texts\n",
      "ID: 2732, Token: workhorse\n",
      "ID: 2733, Token: affects\n",
      "ID: 2734, Token: choice\n",
      "ID: 2735, Token: dialects\n",
      "ID: 2736, Token: distinguish\n",
      "ID: 2737, Token: exploration\n",
      "ID: 2738, Token: operational\n",
      "ID: 2739, Token: addresses\n",
      "ID: 2740, Token: clauses\n",
      "ID: 2741, Token: engines\n",
      "ID: 2742, Token: grouping\n",
      "ID: 2743, Token: instructor\n",
      "ID: 2744, Token: limiting\n",
      "ID: 2745, Token: navigate\n",
      "ID: 2746, Token: statement\n",
      "ID: 2747, Token: cycle\n",
      "ID: 2748, Token: enrolling\n",
      "ID: 2749, Token: phases\n",
      "ID: 2750, Token: conversations\n",
      "ID: 2751, Token: empowered\n",
      "ID: 2752, Token: explaining\n",
      "ID: 2753, Token: novice\n",
      "ID: 2754, Token: programmers\n",
      "ID: 2755, Token: wrangle\n",
      "ID: 2756, Token: adaptable\n",
      "ID: 2757, Token: hourly\n",
      "ID: 2758, Token: noaa\n",
      "ID: 2759, Token: period\n",
      "ID: 2760, Token: precipitation\n",
      "ID: 2761, Token: rates\n",
      "ID: 2762, Token: studio\n",
      "ID: 2763, Token: suited\n",
      "ID: 2764, Token: ten\n",
      "ID: 2765, Token: wisconsin\n",
      "ID: 2766, Token: attention\n",
      "ID: 2767, Token: author\n",
      "ID: 2768, Token: bensouda\n",
      "ID: 2769, Token: brain\n",
      "ID: 2770, Token: decoder\n",
      "ID: 2771, Token: encoder\n",
      "ID: 2772, Token: english\n",
      "ID: 2773, Token: german\n",
      "ID: 2774, Token: helped\n",
      "ID: 2775, Token: kaiser\n",
      "ID: 2776, Token: keras\n",
      "ID: 2777, Token: mourri\n",
      "ID: 2778, Token: please\n",
      "ID: 2779, Token: proficiency\n",
      "ID: 2780, Token: question\n",
      "ID: 2781, Token: reformer\n",
      "ID: 2782, Token: sentences\n",
      "ID: 2783, Token: sequence\n",
      "ID: 2784, Token: staff\n",
      "ID: 2785, Token: stanford\n",
      "ID: 2786, Token: sure\n",
      "ID: 2787, Token: t5\n",
      "ID: 2788, Token: tensor2tensor\n",
      "ID: 2789, Token: transformer\n",
      "ID: 2790, Token: trax\n",
      "ID: 2791, Token: younes\n",
      "ID: 2792, Token: corpus\n",
      "ID: 2793, Token: embeddings\n",
      "ID: 2794, Token: gated\n",
      "ID: 2795, Token: generate\n",
      "ID: 2796, Token: glove\n",
      "ID: 2797, Token: gru\n",
      "ID: 2798, Token: layers\n",
      "ID: 2799, Token: lstm\n",
      "ID: 2800, Token: lstms\n",
      "ID: 2801, Token: ner\n",
      "ID: 2802, Token: recurrent\n",
      "ID: 2803, Token: shakespeare\n",
      "ID: 2804, Token: siamese\n",
      "ID: 2805, Token: synthetic\n",
      "ID: 2806, Token: word\n",
      "ID: 2807, Token: worded\n",
      "ID: 2808, Token: auto\n",
      "ID: 2809, Token: bag\n",
      "ID: 2810, Token: continuous\n",
      "ID: 2811, Token: correct\n",
      "ID: 2812, Token: distance\n",
      "ID: 2813, Token: edit\n",
      "ID: 2814, Token: gram\n",
      "ID: 2815, Token: linguistics\n",
      "ID: 2816, Token: n\n",
      "ID: 2817, Token: pos\n",
      "ID: 2818, Token: probabilistic\n",
      "ID: 2819, Token: tagging\n",
      "ID: 2820, Token: vital\n",
      "ID: 2821, Token: viterbi\n",
      "ID: 2822, Token: word2vec\n",
      "ID: 2823, Token: words\n",
      "ID: 2824, Token: commenting\n",
      "ID: 2825, Token: generic\n",
      "ID: 2826, Token: conjunction\n",
      "ID: 2827, Token: difficult\n",
      "ID: 2828, Token: especially\n",
      "ID: 2829, Token: extremely\n",
      "ID: 2830, Token: figures\n",
      "ID: 2831, Token: fill\n",
      "ID: 2832, Token: guidance\n",
      "ID: 2833, Token: her\n",
      "ID: 2834, Token: intimidating\n",
      "ID: 2835, Token: limited\n",
      "ID: 2836, Token: quantitative\n",
      "ID: 2837, Token: reproducible\n",
      "ID: 2838, Token: routinely\n",
      "ID: 2839, Token: say\n",
      "ID: 2840, Token: sort\n",
      "ID: 2841, Token: worker\n",
      "ID: 2842, Token: numeric\n",
      "ID: 2843, Token: rule\n",
      "ID: 2844, Token: sampling\n",
      "ID: 2845, Token: scope\n",
      "ID: 2846, Token: 1st\n",
      "ID: 2847, Token: assumption\n",
      "ID: 2848, Token: balance\n",
      "ID: 2849, Token: bonds\n",
      "ID: 2850, Token: earnings\n",
      "ID: 2851, Token: excellent\n",
      "ID: 2852, Token: expected\n",
      "ID: 2853, Token: extend\n",
      "ID: 2854, Token: lasso\n",
      "ID: 2855, Token: must\n",
      "ID: 2856, Token: regressions\n",
      "ID: 2857, Token: ridge\n",
      "ID: 2858, Token: stocks\n",
      "ID: 2859, Token: toward\n",
      "ID: 2860, Token: ggplot\n",
      "ID: 2861, Token: ggplot2\n",
      "ID: 2862, Token: hopkins\n",
      "ID: 2863, Token: johns\n",
      "ID: 2864, Token: polish\n",
      "ID: 2865, Token: scenes\n",
      "ID: 2866, Token: works\n",
      "ID: 2867, Token: console\n",
      "ID: 2868, Token: contributions\n",
      "ID: 2869, Token: fluency\n",
      "ID: 2870, Token: individually\n",
      "ID: 2871, Token: rigorous\n",
      "ID: 2872, Token: settings\n",
      "ID: 2873, Token: textual\n",
      "ID: 2874, Token: tidy\n",
      "ID: 2875, Token: upon\n",
      "ID: 2876, Token: ajax\n",
      "ID: 2877, Token: automatically\n",
      "ID: 2878, Token: certainly\n",
      "ID: 2879, Token: coder\n",
      "ID: 2880, Token: css\n",
      "ID: 2881, Token: desired\n",
      "ID: 2882, Token: devices\n",
      "ID: 2883, Token: directly\n",
      "ID: 2884, Token: expects\n",
      "ID: 2885, Token: ground\n",
      "ID: 2886, Token: interacts\n",
      "ID: 2887, Token: irrelevant\n",
      "ID: 2888, Token: pages\n",
      "ID: 2889, Token: phone\n",
      "ID: 2890, Token: phones\n",
      "ID: 2891, Token: pinch\n",
      "ID: 2892, Token: poorly\n",
      "ID: 2893, Token: rearrange\n",
      "ID: 2894, Token: resize\n",
      "ID: 2895, Token: size\n",
      "ID: 2896, Token: tablets\n",
      "ID: 2897, Token: themselves\n",
      "ID: 2898, Token: ubiquitous\n",
      "ID: 2899, Token: utilizes\n",
      "ID: 2900, Token: zoom\n",
      "ID: 2901, Token: ample\n",
      "ID: 2902, Token: arrays\n",
      "ID: 2903, Token: assessed\n",
      "ID: 2904, Token: assign\n",
      "ID: 2905, Token: interactions\n",
      "ID: 2906, Token: lacking\n",
      "ID: 2907, Token: recommendations\n",
      "ID: 2908, Token: releases\n",
      "ID: 2909, Token: written\n",
      "ID: 2910, Token: aspect\n",
      "ID: 2911, Token: brief\n",
      "ID: 2912, Token: dom\n",
      "ID: 2913, Token: exchange\n",
      "ID: 2914, Token: jquery\n",
      "ID: 2915, Token: oo\n",
      "ID: 2916, Token: oriented\n",
      "ID: 2917, Token: alert\n",
      "ID: 2918, Token: button\n",
      "ID: 2919, Token: canvas\n",
      "ID: 2920, Token: classes\n",
      "ID: 2921, Token: divs\n",
      "ID: 2922, Token: filters\n",
      "ID: 2923, Token: ids\n",
      "ID: 2924, Token: links\n",
      "ID: 2925, Token: onchange\n",
      "ID: 2926, Token: onclick\n",
      "ID: 2927, Token: paragraphs\n",
      "ID: 2928, Token: slider\n",
      "ID: 2929, Token: styles\n",
      "ID: 2930, Token: upload\n",
      "ID: 2931, Token: 16\n",
      "ID: 2932, Token: animation\n",
      "ID: 2933, Token: bootstrap\n",
      "ID: 2934, Token: communication\n",
      "ID: 2935, Token: controlled\n",
      "ID: 2936, Token: es\n",
      "ID: 2937, Token: es6\n",
      "ID: 2938, Token: explores\n",
      "ID: 2939, Token: fetch\n",
      "ID: 2940, Token: flux\n",
      "ID: 2941, Token: preferably\n",
      "ID: 2942, Token: react\n",
      "ID: 2943, Token: reactstrap\n",
      "ID: 2944, Token: redux\n",
      "ID: 2945, Token: responsive\n",
      "ID: 2946, Token: rounds\n",
      "ID: 2947, Token: router\n",
      "ID: 2948, Token: strongly\n",
      "ID: 2949, Token: tour\n",
      "ID: 2950, Token: ver\n",
      "ID: 2951, Token: billion\n",
      "ID: 2952, Token: browsers\n",
      "ID: 2953, Token: entertainment\n",
      "ID: 2954, Token: interact\n",
      "ID: 2955, Token: webpages\n",
      "ID: 2956, Token: breaking\n",
      "ID: 2957, Token: deleting\n",
      "ID: 2958, Token: interactivity\n",
      "ID: 2959, Token: larger\n",
      "ID: 2960, Token: manageable\n",
      "ID: 2961, Token: modifying\n",
      "ID: 2962, Token: objectives\n",
      "ID: 2963, Token: pieces\n",
      "ID: 2964, Token: plugins\n",
      "ID: 2965, Token: populate\n",
      "ID: 2966, Token: smaller\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "# Function to tokenize text\n",
    "def tokenize_course(text, lowercase):\n",
    "    import re\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # Simple tokenization\n",
    "    return tokens\n",
    "\n",
    "# Tokenize each course text\n",
    "course_content_df['tokens'] = course_content_df['course_texts'].apply(lambda text: tokenize_course(text, True))\n",
    "\n",
    "# Display the DataFrame with tokens\n",
    "print(\"Courses with Tokenized Texts:\\n\", course_content_df[['COURSE_ID', 'TITLE', 'tokens']])\n",
    "\n",
    "# Extract tokenized texts into a list of lists\n",
    "tokenized_courses = course_content_df['tokens'].tolist()\n",
    "\n",
    "# Create the token dictionary using gensim\n",
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)\n",
    "\n",
    "# Print the dictionary to see its contents\n",
    "print(\"Token Dictionary:\")\n",
    "for token_id, token in tokens_dict.items():\n",
    "    print(f\"ID: {token_id}, Token: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use `doc2bow()` method to generate BoW features for each tokenized course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Use tokens_dict.doc2bow() to generate BoW features for each tokenized course._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses with BoW Features:\n",
      "       COURSE_ID                                              TITLE  \\\n",
      "0      ML0201EN  robots are coming  build iot apps with watson ...   \n",
      "1      ML0122EN                accelerating deep learning with gpu   \n",
      "2    GPXX0ZG0EN  consuming restful services using the reactive ...   \n",
      "3      RP0105EN         analyzing big data in r using apache spark   \n",
      "4    GPXX0Z2PEN  containerizing  packaging  and running a sprin...   \n",
      "..          ...                                                ...   \n",
      "302  excourse89                       javascript  jquery  and json   \n",
      "303  excourse90  programming foundations with javascript  html ...   \n",
      "304  excourse91               front end web development with react   \n",
      "305  excourse92                    introduction to web development   \n",
      "306  excourse93           interactivity with javascript and jquery   \n",
      "\n",
      "                                          bow_features  \n",
      "0    [(0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 1...  \n",
      "1    [(1, 1), (3, 2), (4, 1), (5, 1), (6, 5), (8, 1...  \n",
      "2    [(1, 1), (22, 1), (40, 1), (44, 1), (46, 2), (...  \n",
      "3    [(1, 3), (13, 4), (19, 1), (46, 1), (52, 1), (...  \n",
      "4    [(1, 2), (5, 1), (6, 2), (22, 1), (46, 1), (87...  \n",
      "..                                                 ...  \n",
      "302  [(1, 3), (2, 1), (6, 5), (13, 1), (17, 1), (19...  \n",
      "303  [(0, 1), (1, 6), (2, 1), (5, 1), (6, 8), (9, 1...  \n",
      "304  [(1, 5), (2, 2), (5, 1), (6, 8), (17, 2), (19,...  \n",
      "305  [(1, 8), (5, 1), (6, 8), (8, 3), (9, 1), (12, ...  \n",
      "306  [(1, 1), (6, 6), (12, 2), (13, 3), (17, 1), (1...  \n",
      "\n",
      "[307 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "# Generate BoW features for each tokenized course\n",
    "def generate_bow_features(tokenized_course):\n",
    "    return tokens_dict.doc2bow(tokenized_course)\n",
    "\n",
    "# Apply the function to each course\n",
    "course_content_df['bow_features'] = course_content_df['tokens'].apply(generate_bow_features)\n",
    "\n",
    "# Display the DataFrame with BoW features\n",
    "print(\"Courses with BoW Features:\\n\", course_content_df[['COURSE_ID', 'TITLE', 'bow_features']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "    \n",
    "You can use `tokens_dict.doc2bow(course)` command  for each course in `tokenized_courses`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you need to append the BoW features for each course into a new BoW dataframe. The new dataframe needs to include the following columns (you may include other relevant columns as well):\n",
    "- 'doc_index': the course index starting from 0\n",
    "- 'doc_id': the actual course id such as `ML0201EN`\n",
    "- 'token': the tokens for each course\n",
    "- 'bow': the bow value for each token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Create a new course_bow dataframe based on the extracted BoW features._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course BoW DataFrame:\n",
      "        doc_index      doc_id       token  bow\n",
      "0              0    ML0201EN           2    1\n",
      "1              0    ML0201EN           a    2\n",
      "2              0    ML0201EN       about    1\n",
      "3              0    ML0201EN          ai    2\n",
      "4              0    ML0201EN       along    1\n",
      "...          ...         ...         ...  ...\n",
      "15443        306  excourse93  objectives    1\n",
      "15444        306  excourse93      pieces    1\n",
      "15445        306  excourse93     plugins    1\n",
      "15446        306  excourse93    populate    1\n",
      "15447        306  excourse93     smaller    1\n",
      "\n",
      "[15448 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "# Initialize list to store rows for the new DataFrame\n",
    "rows = []\n",
    "\n",
    "# Create the new DataFrame with doc_index, doc_id, token, and bow\n",
    "for doc_index, doc_bow in enumerate(course_content_df['bow_features']):\n",
    "    doc_id = course_content_df['COURSE_ID'].iloc[doc_index]\n",
    "    for token_index, token_bow in doc_bow:\n",
    "        token = tokens_dict[token_index]  # Get the token from the dictionary\n",
    "        rows.append({\n",
    "            'doc_index': doc_index,\n",
    "            'doc_id': doc_id,\n",
    "            'token': token,\n",
    "            'bow': token_bow\n",
    "        })\n",
    "\n",
    "# Create the DataFrame from the rows\n",
    "course_bow_df = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame with BoW features\n",
    "print(\"Course BoW DataFrame:\\n\", course_bow_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your course BoW dataframe may look like the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may refer to previous code examples in this lab if you need help with creating the BoW dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other popular textual features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the basic token BoW feature, there are two other types of widely used textual features. If you are interested, you may explore them yourself to learn how to extract them from the course textual content: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf-idf**: tf-idf refers to Term Frequency–Inverse Document Frequency. Similar to BoW, the tf-idf also counts the word frequencies in each document. Furthermore, tf-idf will  offset the number of documents in the corpus that contain the word in order to adjust for the fact that some words appear more frequently in general. The higher the tf-idf normally means the greater the importance the word/token is.\n",
    "- **Text embedding vector**. Embedding means projecting an object into a latent feature space. We normally employ neural networks or deep neural networks to learn the latent features of a textual object such as a word, a sentence, or the entire document. The learned latent feature vectors will be used to represent the original textual entities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have completed the BoW feature extraction lab. In this lab, you have learned and practiced extracting BoW features from course titles and descriptions. Once the feature vectors on the courses has been built, we can then apply machine learning algorithms such as similarity measurements, clustering, or classification on the courses in later labs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle## Change Log\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "```\n",
    "```toggle|-|-|-|-|\n",
    "```\n",
    "```toggle|2021-10-25|1.0|Yan|Created the initial version|\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "8d069c20f6b67c72893be20759006a23e5dfcb0c7f9686cfba351e882ac276d0"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
